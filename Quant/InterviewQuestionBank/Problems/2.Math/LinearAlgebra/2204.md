# Metadata
> Title: QR Decomposition

> Difficulty: Easy

> Category: Mathematics/Linear Algebra/Matrix Decomposition

> Tags: Trace; QR Decomposition

> Source: NA

# Question
How do you use QR decomposition in linear regression?

# Answer
Recall that the normal equation 
\begin{align}
    \beta = (X^TX)^{-1} X^T y
\end{align}
involves the inverse of $X^TX$, which is numerically unstable when the matrix is near singular and ill-conditioned, i.e., the condition number $\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = ||A^{-1}| \cdot |||A||$ is large. Now, suppose we have a thin QR factorization of the design matrix (usually $n \ge p$) $X_{n \times p} = QR = [Q_1, Q_2]\begin{pmatrix}R_1\\ 0\end{pmatrix} = Q_1R_1,$ where $Q_1, Q_2$ both have orthonormal columns but with sizes $n \times p$ and $n \times (n-p)$, respectively. $R_1$ is an upper triangular matrix with size $p \times p$.  Then since orthogonal matrices preserve the 2-norm, we have:
\begin{align}
    &\min_\beta \frac12||X\beta - y||_2^2 \\
    \implies & \min_\beta \frac12||Q_1R_1\beta - y||_2^2\\
    \implies & \min_\beta \frac12||Q_1^T(Q_1R_1\beta - y)||_2^2\\
    \implies & \min_\beta \frac12||R_1\beta - Q_1^Ty||_2^2\\		
\end{align}
Since $R_1$ is upper triangular, we only need one backward sweep and one forward sweep to solve $\beta$.
