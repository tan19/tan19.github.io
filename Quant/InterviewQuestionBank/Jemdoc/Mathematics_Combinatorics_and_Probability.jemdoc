# jemdoc: menu{MENU_Interview}{Mathematics_Combinatorics_and_Probability.html}
= Combinatorics and Probability (13 Problems)

- Coin Games (1 Problems)
- Conditional Probability (1 Problems)
- Deterministic Games (6 Problems)
- Expectation (4 Problems)
- Uniform Random Variables (1 Problems)
== Coin Games

~~~
*Question \#1: *
You and I each flip 3 fair coins, if we got same heads I pay you \$2, if different you pay me \$1. Will you play this game?

\n\n*Title: *Coin Game\n*FileName: *2111.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Coin Games\n*Tags: *Coin Games\n*Source: *SIG\n
~~~

*Answer: *
Naive Way:
\(
\begin{align}
	P(A=B=0) &= \left[\binom{3}{0} \left(\frac{1}{2}\right)^3\right]^2 = \frac{1}{64}\\
	P(A=B=1) &= \left[\binom{3}{1} \left(\frac{1}{2}\right)^3\right]^2 = \frac{9}{64}\\
	P(A=B=2) &= \left[\binom{3}{2} \left(\frac{1}{2}\right)^3\right]^2 = \frac{9}{64}\\
	P(A=B=3) &= \left[\binom{3}{3} \left(\frac{1}{2}\right)^3\right]^2 = \frac{1}{64}	
\end{align}
\)

So
\(
\begin{align}
	P(A=B) &= \frac{1}{64} + \frac{9}{64} + \frac{9}{64} + \frac{1}{64} = \frac{5}{16}
\end{align}
\)

Don't forget the case when both had no head (A=B=0).

Clever Way: Let both do their tossing and then let B turn over each of her coins. Then the event you are looking for is that exactly three out of six coins show heads. Since B's "trick" doesn't destroy any randomness or independency, the answer is
\(
\begin{align}
	P(A=B) &= \binom{6}{3} \left(\frac{1}{2}\right)^3 = \frac{5}{16}
\end{align}
\)



== Conditional Probability

~~~
*Question \#2: *
In a hospital there were 3 boys and some girls. A woman gave birth to a child in the hospital. A nurse picked up a child at random and was a boy. What is the probability that that woman gave birth to a boy?

\n\n*Title: *Bayesian Child\n*FileName: *2103.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Conditional Probability\n*Tags: *Bayes Theorem; Conditional Probability\n*Source: *SIG\n
~~~

*Answer: *
\(
\begin{align}
	P(\text{Boy} ~|~ \text{Picked a Boy}) &= \frac{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy})}{P(\text{Picked a Boy})}\\
	 &= \frac{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy})}{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy}) + P(\text{Picked a Boy} ~|~ \text{Girl})P(\text{Girl})}\\
	 &= \frac{\frac{4}{3+N+1} \cdot \frac{1}{2}}{\frac{4}{3+N+1} \cdot \frac{1}{2} + \frac{3}{3+N+1} \cdot \frac{1}{2}}\\
	 &= \frac{4}{4+3} = \frac{4}{7}
\end{align}
\)


== Deterministic Games

~~~
*Question \#3: *
In a room there are half H and half T. A robot comes into the room and flips the Hs and tosses the Ts, repeatedly. What is the stationary distribution of the Hs and Ts in the room?

\n\n*Title: *Robot Flipping Coins\n*FileName: *2114.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Deterministic Games; Markov Chain\n*Source: *Optiver\n
~~~

*Answer: *
The answer is 1/3 for H and 2/3 for T. We model this as a Markov Chain, which has a transition matrix
\(
\begin{align}
    M = 
    \begin{pmatrix}
        0 & 1\\
        0.5 & 0.5
    \end{pmatrix}	
\end{align}
\)

Suppose in equilibrium, the probability of H is $p$ and the probability of T is $1-p$, then
\(
\begin{align}
    (p, 1-p) 
    \begin{pmatrix}
        0 & 1\\
        0.5 & 0.5
    \end{pmatrix}
    =
    (p, 1-p)
\end{align}
\)

This gives us $p = 1/3$. Alternatively, using eigendecomposition ($M = Q \Lambda Q^{-1}$), we find:
\(
\begin{align}
    M = 
    \begin{pmatrix}
    1 & 1\\
    1 & -1/2\\		
    \end{pmatrix}
    \begin{pmatrix}
    1 & 0\\
    0 & -0.5\\		
    \end{pmatrix}
    \begin{pmatrix}
    1/3 & 2/3\\
    2/3 & -2/3\\		
    \end{pmatrix}
\end{align}
\)

and 
\(
\begin{align}
    M^\infty = 
    \begin{pmatrix}
    1 & 1\\
    1 & -1/2\\		
    \end{pmatrix}
    \begin{pmatrix}
    1 & 0\\
    0 & 0\\		
    \end{pmatrix}
    \begin{pmatrix}
    1/3 & 2/3\\
    2/3 & -2/3\\		
    \end{pmatrix}
\end{align}
\)

This gives us the same result: $(0.5, 0.5) M^\infty = (1/3, 2/3)$.

~~~
*Question \#4: *
Expected number of die rolls until all numbers are rolled? If I have already made $N$ rolls, what is the expected number of distinctive numbers?

\n\n*Title: *Coupon Collection\n*FileName: *2115.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Dice Games; Deterministic Games\n*Source: *Green Book; Black Book; Jump;\n
~~~

*Answer: *
This is the famous ``Coupon Collection'' problem. Define a set of variables $X_1, \dots, X_6$ where $X_i$ denotes the number of additional rolls need to obtain the $i^{th}$ new number, then the answer reduces to computing $E[\sum_{i=1}^6 X_i]$. However, each $X_i$ follows a Geometric distribution where $p_i = (6 - (i-1)) / 6$ and $E[X_i] = 6 / (6 - (i-1))$. Therefore,
\(
\begin{align}
    E\left[\sum_{i=1}^6 X_i\right] = \sum_{i=1}^6 \frac{6}{6- (i-1)} = 6(1/6 + 1/5 + 1/4 + 1/3 + 1/2 + 1) = 14.7
\end{align}
\)

In general, the answer for the coupon collection problem is:
\(
\begin{align}
E = N\sum_{i=1}^N \frac{1}{i}
\end{align}
\)


For the second part, introduce the indicator variables $Y_i$ to denote that if number $i$ is in the rolls already made. Then
\(
\begin{align}
E\left[\sum_{i=1}^6 Y_i \right] = \sum_{i=1}^6 E[Y_i] = \sum_{i=1}^6 Pr(Y_i = 1) = \sum_{i=1}^6 1 - \left(\frac56\right)^N = 6\left[1 - \left(\frac56\right)^N\right]
\end{align}
\)


~~~
*Question \#5: *
The expected number of flips to see two heads from a series of fair coin tosses.

\n\n*Title: *Two Heads\n*FileName: *2116.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Think Backwards; Symmetry; Coin Games\n*Source: *NA\n
~~~

*Answer: *
Answer is 4 for to see two heads, and 6 to see two consecutive heads. Expected \# of flips to gets 1 H is just, $E[X] = 1 + (1-p)(E[X])$, where $p$ is prob. of getting H, in the case of a fair coin this is 0.5, therefore rearranging you get $E[X] = 1/p$, meaning expected number of flips for 1 head is $1/p$, for fair coin this is equal to 2. Since coin flipping is memoryless, expected number of flips to get $k$ heads is just $1/p + 1/p + ... = k/p$. Therefore 2/0.4 = 4, when $k = 2$.

~~~
*Question \#6: *
A tosses a fair coin $n+1$ times; B tosses a fair coin $n$ times. What is the probability that A has strictly more heads than B?

\n\n*Title: *One More Coin\n*FileName: *2117.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Think Backwards; Symmetry; Coin Games\n*Source: *Green Book; Blue Book; HRT;\n
~~~

*Answer: *
The answer is 1/2. First observe that if A were to toss only $n$ times, then there are only three events:
- With probability $P_1$, A has more heads than B does.
- With Probability $P_2$, A has the same number of heads compared to B.
- With probability $P_3$, A has fewer heads than B does.

By symmetry, $P_1 = P_3$ and $P_1 + P_2 + P_3 = 1 \implies 2P_1 + P_2 = 1$. The additional coin toss A has increases the probability of A having more heads by just $0.5P_3$, hence the probability in question is $P_1 + 0.5P_2 = 0.5$.

~~~
*Question \#7: *
Toss a die 10 times. What is the probability that the sum of the 10 numbers you get can be divided by 7? How about the probability of tossing a die with $F$ sides and the sum of $m$ numbers you get can be divided by $N$ with remainder $k$?

\n\n*Title: *Sum of Dice\n*FileName: *2118.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Think Backwards; Symmetry; Die Games\n*Source: *Egret Asset\n
~~~

*Answer: *


~~~
*Question \#8: *
A and B play table tennis. A has the probability of 1/3 to win a round. Suppose A wins the game if he can win 5 rounds but B wins the game only if he can win 11 rounds. What is the probability of A winning the game? What if, when the game is 10:4, one side has to win two more points to win the game (say, 12:4 or 10:6), what is the probability of A winning the game?

\n\n*Title: *Table Tennis\n*FileName: *2119.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Deterministic Games\n*Tags: *Probability\n*Source: *Egret Asset\n
~~~

*Answer: *


== Expectation

~~~
*Question \#9: *
What is the expected number of tosses you need to see all six sides of a die?

\n\n*Title: *All Six Sides of A Die\n*FileName: *2101.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Expectation\n*Tags: *Expectation\n*Source: *NA\n
~~~

*Answer: *
The answer is $6 \cdot \sum_{i=1}^6 \frac{1}{i}$. Let $X_i$ denote the random variable that tells us the number of trials it takes for us to see the i-th distinct side on a die. Obviously, $X_1$ is always 1, since the first roll gives us the first observed side. $X_2$ follows a geometric distribution with success p = 5/6. Similarly $X_i$ follows a geometric distribution with success $p = (7-i)/6$. So, the total expected number of rolls is $E[X_1 + ... + X_6] = E[X_1] + ... E[X_6] = 6 \times (1 + 1/2 + ... + 1/6)$.

~~~
*Question \#10: *
You turn over a card one by one from a deck of 52 cards. What is the expected number of cards that you need to flip before you see the first ace?

\n\n*Title: *First Ace\n*FileName: *2102.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Expectation\n*Tags: *Expectation\n*Source: *Green Book, page 95\n
~~~

*Answer: *
The answer is 10.6. Setup an indicator function $X_i = \mbox{all aces are behind card } i$. Then total number of draws is $N = 1 + \sum_{i=1}^48 X_i$, where $p(X_i) = 1/5$. The 1/5 comes from the fact that each non-ace card has a probability of 1/5 being appear before all aces.

~~~
*Question \#11: *
There are $x$ people from country A and y people from country B. They sit around a table and shake hands with people on their left and right, but they only shake hands if they're from the same country. We want to know the expected number of handshake made.

\n\n*Title: *Hand Shakes\n*FileName: *2112.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Expectation\n*Tags: *Indicator Variables; Linearity of Expectations\n*Source: *NA\n
~~~

*Answer: *
For each seat, the probability the person sitting there shakes the hand to the left is
\(
\begin{align}
\frac{x}{x+y} \cdot \frac{x-1}{x+y-1} + \frac{y}{x+y} \cdot \frac{y-1}{x+y-1} = \frac{x(x-1) + y(y-1)}{(x+y)(x+y-1)}
\end{align}
\)

Hence
\(
\begin{align}
E\left(\sum_i Z_i\right) = \sum_i Z_i = (x+y) \cdot \frac{x(x-1) + y(y-1)}{(x+y)(x+y-1)} = \frac{x(x-1) + y(y-1)}{x+y-1}
\end{align}
\)


~~~
*Question \#12: *
There are 10 hunters and 10 ducks flying over. Each hunter aims at one duck and there is a probability of 0.5 that a hunter kills the duck he aims at. What is the expected number of ducks that have survived?

\n\n*Title: *Hunters and Ducks\n*FileName: *2113.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Expectation\n*Tags: *Indicator Variables; Linearity of Expectations\n*Source: *Morgan Stanley\n
~~~

*Answer: *
The answer is 6 (but mine is 5?). Define a set of indicator variables:
\(
\begin{align}
    Z_i = \begin{cases} 
    0, ~~ \mbox{if the ith duck has survived}\\
    1, ~~ \mbox{if the ith duck has died}		
    \end{cases}
\end{align}
\)

where $i = 1, \dots, 10$. Additionally, we define $Pr(Y_j = i)$ to be the probability that the jth hunter aims at the ith duck. Now,
\(
\begin{align}
    E\left[\sum_{i=1}^{10} Z_i\right] &= \sum_{i=1}^{10} E[Z_i] = \sum_{i=1}^{10} Pr(Z_i=1)\\
    &= \sum_{i=1}^{10} \sum_{j=1}^{10} Pr(Z_i=1|Y_j = i) Pr(Y_j = i)\\
    &= \sum_{i=1}^{10} \sum_{j=1}^{10} (1/2)(1/10)\\
    &= 5		
\end{align}
\)


== Uniform Random Variables

~~~
*Question \#13: *
$X, Y, Z$ are i.i.d. $U[0,1]$. What is the distribution of $(XY)^Z$?

\n\n*Title: *Uniform Random Variables\n*FileName: *2104.md\n*Difficulty: *Easy\n*Category: *Mathematics\/Combinatorics and Probability\/Uniform Random Variables\n*Tags: *Uniform Random Variables\n*Source: *NA\n
~~~

*Answer: *


