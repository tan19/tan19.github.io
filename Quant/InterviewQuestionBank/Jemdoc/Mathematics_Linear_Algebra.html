<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Linear Algebra (4 Problems)</title>
<!-- Customized Scripts -->
<link rel="stylesheet" href="http://tan19.github.io/bin/jemdoc.css", type="text/css">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End Customized Scripts -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Quant</div>
<div class="menu-item"><a href="../index.html">Go&nbsp;Up</a></div>
<div class="menu-item"><a href="http://tan19.github.io">Homepage</a></div>
<div class="menu-category">Brainteaser</div>
<div class="menu-item"><a href="../../Brainteaser.html">Introduction</a></div>
<div class="menu-item"><a href="Brainteaser_Logic_Brainteaser.html">Logic&nbsp;Brainteaser</a></div>
<div class="menu-item"><a href="Brainteaser_Math_Brainteaser.html">Math&nbsp;Brainteaser</a></div>
<div class="menu-category">Math</div>
<div class="menu-item"><a href="../../Math.html">Introduction</a></div>
<div class="menu-item"><a href="Mathematics_Combinatorics_and_Probability.html">Combinatorics&nbsp;&amp;&nbsp;Probability</a></div>
<div class="menu-item"><a href="Mathematics_Linear_Algebra.html" class="current">Linear&nbsp;Algebra</a></div>
<div class="menu-item"><a href="Mathematics_Calculus.html">Calculus</a></div>
<div class="menu-item"><a href="Mathematics_Differential_Equations.html">Differential&nbsp;Equations</a></div>
<div class="menu-category">Finance</div>
<div class="menu-item"><a href="../../Finance.html">Introduction</a></div>
<div class="menu-item"><a href="Finance_Stochastic_Calculus.html">Stochastic&nbsp;Calculus</a></div>
<div class="menu-item"><a href="Finance_Trading.html">Options&nbsp;Trading&nbsp;Strategies</a></div>
<div class="menu-item"><a href="*">Implied&nbsp;Vol&nbsp;Surface&nbsp;Fitting</a></div>
<div class="menu-item"><a href="*">Equity&nbsp;Derivatives&nbsp;Pricing</a></div>
<div class="menu-category">Computer Science</div>
<div class="menu-item"><a href="../../CS.html">Introduction</a></div>
<div class="menu-item"><a href="Computer_Science_Programming_Languages.html">Programming&nbsp;Languages</a></div>
<div class="menu-item"><a href="Computer_Science_Compiler.html">Compiler</a></div>
<div class="menu-item"><a href="Computer_Science_Database.html">Database</a></div>
<div class="menu-item"><a href="Computer_Science_Operating_System.html">Operating&nbsp;System</a></div>
<div class="menu-item"><a href="Computer_Science_Network.html">Network</a></div>
<div class="menu-item"><a href="Computer_Science_Design.html">Design</a></div>
<div class="menu-category">Algorithms and Optimization</div>
<div class="menu-item"><a href="../../Algorithm.html">Introduction</a></div>
<div class="menu-item"><a href="Algorithm_Data_Structures.html">Data&nbsp;Structures</a></div>
<div class="menu-item"><a href="Algorithm_Algorithms.html">Algorithms</a></div>
<div class="menu-item"><a href="Algorithm_Unconstrained_Optimization.html">Unconstrained&nbsp;Optimization</a></div>
<div class="menu-item"><a href="Algorithm_Constrained_Optimization.html">Constrained&nbsp;Optimization</a></div>
<div class="menu-category">Machine Learning</div>
<div class="menu-item"><a href="../../ML.html">Introduction</a></div>
<div class="menu-item"><a href="*">Mathematical&nbsp;Statistics</a></div>
<div class="menu-item"><a href="*">Linear&nbsp;Regression</a></div>
<div class="menu-item"><a href="*">Non-Linear&nbsp;Regression</a></div>
<div class="menu-item"><a href="*">Linear&nbsp;Classification</a></div>
<div class="menu-item"><a href="*">Non-Linear&nbsp;Classification</a></div>
<div class="menu-item"><a href="*">Deep&nbsp;Learning</a></div>
<div class="menu-item"><a href="Machine_Learning_Models.html">Models</a></div>
<div class="menu-category">Projects</div>
<div class="menu-item"><a href="../../LeetCode/index.html">LeetCode</a></div>
<div class="menu-item"><a href="*">Kaggle</a></div>
<div class="menu-item"><a href="*">Github</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Linear Algebra (4 Problems)</h1>
<div id="subtitle"></div>
</div>
<ul>
<li><p>Determinant (1 Problems)</p>
</li>
<li><p>Matrix Decomposition (1 Problems)</p>
</li>
<li><p>Positive Definiteness and Covariance Matrices (1 Problems)</p>
</li>
<li><p>Trace (1 Problems)</p>
</li>
</ul>
<h2>Determinant</h2>
<p></p>
<div class="infoblock">
<div class="blockcontent">
<p><b>Question #1: </b>
Prove the Sherman-Morrison formula: \((I + uv^T)^{-1} = I - \frac{uv^T}{1 + uv^T}\) and the Matrix Determinant Lemma: \(\det(I + uv^T) = 1+v^Tu.\)

<br /><br /><b>Title: </b>Sherman-Morrison Formula<br /><b>FileName: </b>2203.md<br /><b>Difficulty: </b>Easy<br /><b>Category: </b>Mathematics/Linear Algebra/Determinant<br /><b>Tags: </b>Calculus; Sherman-Morrison Formula<br /><b>Source: </b>World Quant<br /></p>
</div></div>
<p><b>Answer: </b>
We guess the inverse has the similar form \((I + uv^T)^{-1} = I + \alpha v^Tu\), thus</p>
<p style="text-align:center">
\[
\begin{align}
(I + uv^T)(I + \alpha uv^T) &amp;= I + auv^T + uv^T + auv^Tuv^T = I + u(a+1+av^Tu)v^T \\
&amp;\implies a = -\frac{uv^T}{1+u^Tv}
\end{align}
\]
</p><p>
and hence \((I + uv^T)^{-1} = I - \frac{uv^T}{1 + u^Tv}.\)

If we look at the eigenvalues,</p>
<p style="text-align:center">
\[
\begin{align}
    (I + uv^T)u = u + uv^Tu = (1 + v^Tu)u
\end{align}
\]
</p><p>
We see that \(u\) is an eigenvector and \(1 + v^Tu\) is its eigenvalue. Moreover, there are \(n-1\) orthogonal vectors (suppose \(I\) is of dimension \(n\)) \(b\) such that \(v^T b = 0\), and for each of them we have \((I + uv^T)b = b\), which suggests that \(1\) is the other eigenvalue with multiplicity \(n-1\). Therefore, the determinant of \(I + uv^T\) is the product of its eigenvalues, which is \(1 + u^Tv\). That is</p>
<p style="text-align:center">
\[
\begin{align}
    \det(I + uv^T) = 1 + u^Tv
\end{align}
\]
</p><p>
This proves the Matrix Determinant Lemma.
</p>
<h2>Matrix Decomposition</h2>
<p></p>
<div class="infoblock">
<div class="blockcontent">
<p><b>Question #2: </b>
How do you use QR decomposition in linear regression?

<br /><br /><b>Title: </b>QR Decomposition<br /><b>FileName: </b>2204.md<br /><b>Difficulty: </b>Easy<br /><b>Category: </b>Mathematics/Linear Algebra/Matrix Decomposition<br /><b>Tags: </b>Trace; QR Decomposition<br /><b>Source: </b>NA<br /></p>
</div></div>
<p><b>Answer: </b>
Recall that the normal equation </p>
<p style="text-align:center">
\[
\begin{align}
    \beta = (X^TX)^{-1} X^T y
\end{align}
\]
</p><p>
involves the inverse of \(X^TX\), which is numerically unstable when the matrix is near singular and ill-conditioned, i.e., the condition number \(\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} = ||A^{-1}| \cdot |||A||\) is large. Now, suppose we have a thin QR factorization of the design matrix (usually \(n \ge p\)) \(X_{n \times p} = QR = [Q_1, Q_2]\begin{pmatrix}R_1\\ 0\end{pmatrix} = Q_1R_1,\) where \(Q_1, Q_2\) both have orthonormal columns but with sizes \(n \times p\) and \(n \times (n-p)\), respectively. \(R_1\) is an upper triangular matrix with size \(p \times p\).  Then since orthogonal matrices preserve the 2-norm, we have:</p>
<p style="text-align:center">
\[
\begin{align}
    &amp;\min_\beta \frac12||X\beta - y||_2^2 \\
    \implies &amp; \min_\beta \frac12||Q_1R_1\beta - y||_2^2\\
    \implies &amp; \min_\beta \frac12||Q_1^T(Q_1R_1\beta - y)||_2^2\\
    \implies &amp; \min_\beta \frac12||R_1\beta - Q_1^Ty||_2^2\\		
\end{align}
\]
</p><p>
Since \(R_1\) is upper triangular, we only need one backward sweep and one forward sweep to solve \(\beta\).

</p>
<h2>Positive Definiteness and Covariance Matrices</h2>
<p></p>
<div class="infoblock">
<div class="blockcontent">
<p><b>Question #3: </b>
How to check if a matrix is positive definite? How about positive semi-definite?

<br /><br /><b>Title: </b>How to check positive definiteness of a matrix?<br /><b>FileName: </b>2201.md<br /><b>Difficulty: </b>Easy<br /><b>Category: </b>Mathematics/Linear Algebra/Positive Definiteness and Covariance Matrices<br /><b>Tags: </b>Math, Matrix, Linear Algebra, Positive Definite<br /><b>Source: </b>NA<br /></p>
</div></div>
<p><b>Answer: </b>
Need to check if the matrix is square first.
</p>
<h2>Trace</h2>
<p></p>
<div class="infoblock">
<div class="blockcontent">
<p><b>Question #4: </b>
Given a \(3 \times 3\) matrix \(M\), what is \(\mbox{tr}\left(\sum_{k=0}^\infty M^k\right)\)?

<br /><br /><b>Title: </b>Trace of Matrix Sum<br /><b>FileName: </b>2202.md<br /><b>Difficulty: </b>Easy<br /><b>Category: </b>Mathematics/Linear Algebra/Trace<br /><b>Tags: </b>Trace; Eigenvector; Eigenvalue;<br /><b>Source: </b>NA<br /></p>
</div></div>
<p><b>Answer: </b>
The answer is</p>
<p style="text-align:center">
\[
\begin{align}
\mbox{tr}\left(\sum_{k=0}^\infty M^k\right) = \sum_{i=1}^3 \frac{1}{1 - \lambda_i}
\end{align}
\]
</p><p>
where \(\lambda_i\) are the eigenvalues of \(M\). The key is that trace is the sum of all eigenvalues, and if \(\lambda\) is an eigenvalue of \(M\) then \(\lambda^k\) is an eigenvalue of \(M^k\). This is easily seen by noticing that</p>
<p style="text-align:center">
\[
\begin{align}
	Mx = \lambda x \implies M^2x = \lambda(Mx) = \lambda^2 x
\end{align}
\]
</p><p>
Now, </p>
<p style="text-align:center">
\[
\begin{align}
\mbox{tr}\left(\sum_{k=0}^\infty M^k\right) = \sum_{k=0}^\infty \mbox{tr}(M^k) = \sum_{k=0}^\infty \sum_{i=1}^n \lambda_i^k = \sum_{i=1}^n \sum_{k=0}^\infty \lambda_i^k = \sum_{i=1}^n \frac{1}{1 - \lambda_i}
\end{align}
\]
</p><p>
The answer is given by \(n = 3\).

</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-06-16 09:30:40 Eastern Daylight Time, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
