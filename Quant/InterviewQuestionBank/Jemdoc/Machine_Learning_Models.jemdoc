# jemdoc: menu{MENU_Interview}{Machine_Learning_Models.html}
= Models (3 Problems)

- Machine Learning (3 Problems)
== Machine Learning

~~~
*Question \#1: *
Bayesian v.s. Frequentism?

\n\n*Title: *Bayesian v.s. Frequentism?\n*FileName: *6001.md\n*Difficulty: *Easy\n*Category: *Machine Learning\/Models\/Machine Learning\n*Tags: *Naive Bayes; Logistic Regression\n*Source: *NA\n
~~~

*Answer: *
There are two main schools of statistical inference: Frequentist and Bayesian\footnote{Another being Fiducial inference, or Fisherian inference.}. The controversies arise when it comes to how to interpret the randomness of data point generating process.

Bayesians consider parameters to be \emph{random}, and the observed data are conditioned on a realization of such random variables. Notice the natural hierarchical structure in this interpretation. The goal is to inference $p(\theta \vert D_{\theta_0})$, where $\theta$ are the parameters and $D_{\theta_0}$ the observed data set conditioned on realized values $\theta_0$ of $\theta$. Frequentists consider parameters to be \emph{unknown but fixed}, and the observed data set is just a sample from the population. Efron said "... Bayesian averages involve only the data value $\bar x$ actually seen, rather than a collection of theoretically possible other $\bar x$ values."

Also, as answered by Michael Hochster: Suppose $h$ is the unknown constant, and $H$ is the statistic computed from a sample. For Frequentists, it is valid to write $P(L \le h \le U) = 95\%$ or $P(70 \le H \le 74) = 95\%$, but not $P(70 \le h \le 74) = 95\%$ (this is $0$ or $1$). So the correct way to say is either "if the same experiment procedure is repeated 100 times, 95 times of the CIs will cover the unknown true value $h$", or "before the experiment, the probability is 95\% that the CI to be obtained will cover $h$".

Wasserman said that the two schools of inference differ in their \emph{goals}, not the \emph{methods}: the goal of Frequentist inference is to construct procedures with frequency guarantees, and the goal of Bayesian inference is to quantify and manipulate degrees of beliefs.

~~~
*Question \#2: *
Explain Kalman Filter.

\n\n*Title: *Kalman Filter\n*FileName: *6002.md\n*Difficulty: *Easy\n*Category: *Machine Learning\/Models\/Machine Learning\n*Tags: *Kalman Filter\n*Source: *NA\n
~~~

*Answer: *


~~~
*Question \#3: *
Compare **Naive Bayes** and **Logistic Regression**.

\n\n*Title: *Naive Bayes vs Logistic Regression\n*FileName: *6003.md\n*Difficulty: *Easy\n*Category: *Machine Learning\/Models\/Machine Learning\n*Tags: *Naive Bayes; Logistic Regression\n*Source: *NA\n
~~~

*Answer: *
NB has high bias as it focuses on a smaller set of models, converges faster but error rate may be high if assumptions are incorrect. Logistic Regression has lower bias as it allows a larger model space, converges to solutions slower but error rate can be lower due to low bias. Logistic Regression tends to be used a LOT more in industry but if your prior information is accurate (data is generated from a well known process), NB may be a better choice.

NB assumes that the input features are conditional  independent. NB also requires a prior distribution. Simple LR cannot do nonlinear classification.

