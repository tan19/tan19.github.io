<meta charset="UTF-8">
<html>
<title> Xi Tan's homepage at Purdue University </title>
<body background="index.files/backmath.gif">

<PRE>

<center> ==== Machine Learning ==== </center>


<hr> <!-- a horizontal line divider -->
== PRML ==
1. Introduction
  1.1 Example: Polynomial Curve Fitting
  1.2 Probability Theory
  1.3 Model Selection
  1.4 The Curse of Dimensionality
  1.5 Decision Theory
  1.6 Information Theory

2. Probability Distributions
  2.1 Binary Variables
  2.2 Multinomial Distribution
  2.3 The Gaussian Distribution
  2.4 The Exponential Family
  2.5 Nonparametric Methods

3. Linear Models for Regression
  3.1 Linear Basis Function Models
  3.2 The Bias-Variance Decomposition
  3.3 Bayesian Linear Regression
  3.4 Bayesian Model Comparison
  3.5 The Evidence Approximation
  3.6 Limitations of Fixed Basis Functions

4. Linear Models for Classification
 4.1 Discriminant Functions
 4.2 Probabilistic Generative Models
 4.3 Probabilistic Discriminative Models
 4.4 The Laplace Approximation
 4.5 Bayesian Logistic Regression

5. Neural Networks
  5.1 Feed-forward Network Functions
  5.2 Network Training
  5.3 Error Backpropagation
  5.4 The Hessian Matrix
  5.5 Regularization in Neural Networks
  5.6 Mixture Density Networks
  5.7 Bayesian Neural Networks

6. Kernel Methods
  6.1 Dual Representations
  6.2 Constructing Kernels
  6.3 Radial Basis Function Networks
  6.4 Gaussian Processes

7. Sparse Kernel Machines
  7.1 Maximum Margin Classifiers
  7.2 Relevance Vector Machines

8. Graphical Models
  8.1 Bayesian Networks
  8.2 Conditional Independence
  8.3 Markov Random Fields
  8.4 Inference in Graphical Models

9. Mixture Models and EM
  9.1 K-means Clustering
  9.2 Mixtures of Gaussians
  9.3 An Alternative View of EM
  9.4 The EM Algorithm in General

10. Approximate Inference
  10.1 Variational Inference
  10.2 Illustration: Variational Mixture of Gaussians
  10.3 Variational Linear Regression
  10.4 Exponential Family Distributions
  10.5 Local Variational Methods
  10.6 Variational Logistic Regression
  10.7 Expectation Propagation

11. Sampling Methods
  11.1 Basic Sampling Algorithms
  11.2 Markov Chain Monte Carlo
  11.3 Gibbs Sampling
  11.4 Slice Sampling
  11.5 The Hybrid Monte Carlo Algorithm
  11.6 Estimating the Partition Function

12. Continuous Latent Variables
  12.1 Principle Component Analysis
  12.2 Probabilistic PCA
  12.3 Kernel PCA
  12.4 Nonlinear Latent Variable Models

13. Sequential Data
  13.1 Markov Models
  13.2 Hidden Markov Models
  13.3 Linear Dynamical Systems

14. Combining Models
  14.1 Bayesian Model Averaging
  14.2 Committees
  14.3 Boosting
  14.4 Tree-based Models
  14.5 Conditional Mixture Models






<hr> <!-- a horizontal line divider -->



### Canada ###
- U. of Montreal          : Yoshua Bengio (<i><u>Deep Learning</i></u>)
- U. of Toronto           : Geoffrey Hinton (<i><u>Deep Learning</i></u>), Craig Boutilier, Brendan Frey, Radford Neal (<i><u>MCMC</i></u>)
- U. of Alberta           : Dale Schuurmans

### Europe ###
- U. of Cambridge         : Zoubin Ghahramani, David MacKay, Carl Rasmussen, José Miguel Hernández-Lobato
- U. of Oxford            : Nando DeFrietas, Steffen Lauritzen
- U. of Sheffield         : Neil Lawrence
- U. of Bristol           : Nello Cristianini
- U. of London            : John Shawe-Taylor (<i><u>SVM, Kernel</i></u>), Vladimir Vapnik (<i><u>SVM</i></u>)
- ETH                     : Joachim Buhman
- Max Planck Institute    : Bernhard Schoelkopf, Michael Black
- Hebrew U. of Jerusalem  : Nir Friedman, Yair Weiss
- Microsoft Research (UK) : Christopher Bishop, Tom Minka (<i><u>EP</i></u>), Andrew Blake
- University of Geneva    : Lorenzo Rosasco

### USA ###
== East ==
- UMD                     : Lise Getoor
- Brown                   : Thomas Hoffman, Michael Littman
- Rutgers                 : Tong Zhang, Vladimir Pavlovic
- U. of Mass              : Andrew McCallum
- U. of Penn              : Michael Kearns, Jianbo Shi
- NYU                     : Yann LeCun (<i><u>Deep Learning</i></u>), Sam Roweis, Mehryar Mohri
- Harvard                 : Avi Pfeffer
- Duke                    : Sayan Mukherjee
- CMU                     : Andrew Moore, Tom Mitchell, Alex Smola (Kernel), John Lafferty
- MIT                     : Leslie Kaelbling, Tommi Jaakkola, Bill Freeman, Alex Pentland, Josh Tenenbaum
- Georgia Tech            : Aaron Bobick, Le Song (<i><u>Kernel</i></u>), Irfan Essa, Charles Isbell
- Princeton               : David Blei (<i><u>LDA</i></u>), Rob Schapire
- Cornell                 : Thorsten Joachims, Rich Caruana
- Columbia                : Tony Jebara, Michael Collins, Rocco Servedio, Peter Orbanz

== West ==
- Microsoft Research (US) : David Heckerman, Eric Horovitz, Nebojsa Jojic, Phil Long, Chris Meek, John Platt, Léon Bottou
- Google Research         : Kevin Murphy, Yoram Singer, Samy Bengio, Corinna Cortes, Fernando Perreira
- Stanford                : Andrew Ng (<i><u>Deep Learning</i></u>), Daphne Koller (<i><u>PGM</i></u>)
- UC-Berkeley             : Michael Jordan (<i><u>PGM</i></u>), Peter Bartlett, Leo Breiman, Martin Wainwright
- UW-Seattle              : Marina Meila, Jeff Bilmes, Dieter Fox, Raj Rao
- UCI                     : Padhraic Smyth
- UCSC                    : Manfred Warmuth
- UCSD                    : Yoav Freund, Lawrence Saul
- Oregon State U.         : Tom Dietterich

== Elsewhere ==
- Wisconsin               : Xiaojin Zhu
- U. of Chicago           : Partha Niyogi
- UIUC                    : David Forsyth
- U. of Texas             : Adam Klivans
</div>


### General AI Books ###
- Artificial Intelligence: A Modern Approach, by Stuart Russell and Peter Norvig
- Probability Theory: The Logic of Science, by E. T. Jaynes and G. Larry Bretthorst
- Causality: Models, Reasoning and Inference, by Judea Pearl
- Probabilistic Reasoning in Intelligent Systems, by Judea Pearl

### General ML Books ###
- Machine Learning, by Tom Mitchell
- Learning From Data, by Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin
- Pattern Recognition and Machine Learning, by Christopher M. Bishop
- Pattern Classification, by Richard O. Duda, Peter E. Hart, David G. Stork
- The Elements of Statistical Learning, by T. Hastie, R. Tibshirani, J. H. Friedman
- Bayesian Reasoning and Machine Learning, by David Barber
- Information Theory, Inference and Learning Algorithms, by David Mackay [<a href="http://www.inference.phy.cam.ac.uk/itila/book.html">book</a>,<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video</a>]
- Machine Learning: A Probabilistic Perspective, by Kevin P. Murphy

### Neural Networks ###
- Neural Networks for Pattern Recognition, by Christopher M. Bishop
- Neural Networks: A Comprehensive Foundation, by Simon Haykin
- Neural Network Learning: Theoretical Foundations, by Martin Anthony and Peter L. Bartlett

### SVM ###
- An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, by Nello Cristianini and John Shawe-Taylor
- Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, by Bernhard Schlkopf and Alexander J. Smola

### PGM ###
- Probabilistic Graphical Models: Principles and Techniques, by Daphne Koller and Nir Friedman

### GP ###
- Gaussian Processes for Machine Learning, by Carl E Rasmussen and Christopher K I Williams

### Optimization ###
>> Introduction
- A First Course in Optimization Theory, by Rangarajan K. Sundaram
>> Linear Optimization
- Introduction to Linear Optimization, by Dimitris Bertsimas and John N. Tsitsiklis
>> Non-linear Optimization
- Nonlinear Programming, by Dimitri P. Bertsekas
- Nonlinear Programming: Theory and Algorithms, by Mokhtar S. Bazaraa, Hanif D. Sherali and C. M. Shetty
>> Combinatorial Optimization
- Combinatorial Optimization: Algorithms and Complexity, by Christos H. Papadimitriou and Kenneth Steiglitz
>> Convex Optimization
- Convex Optimization, by Stephen Boyd and Lieven Vandenberghe [<a href="http://web.stanford.edu/~boyd/cvxbook/">book</a>,<a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info">courseware</a>]
- Primal-Dual Interior-Point Methods, by Stephen Wright
- Convex Optimization & Euclidean Distance Geometry, by Jon Dattorro
>> Numerical Optimization
- Numerical Optimization, by Jorge Nocedal and Stephen Wright
- Monte Carlo Statistical Methods, by Christian Robert and George Casella

<hr> <!-- a horizontal line divider -->


                       |-- Introduction            : <a href="https://work.caltech.edu/telecourse.html">Caltech</a>, <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/">AI course</a>
                       |
                       |-- Linear Programming      : <a href="https://class.coursera.org/linearprogramming-001">Coursera</a>
                       |-- Convex Optimization     : <a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/">Stanford</a>  
                       |
                       |-- VB                      : <i><a href="http://www.variational-bayes.org">Repository</a></i>
                       |-- EP                      : <i><a href="http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html">Roadmap</a></i>                       
                       |-- MCMC                    : <i><a href="http://onionesquereality.wordpress.com/2008/08/31/demystifying-support-vector-machines-for-beginners/">Blog</a>, <a href="http://www.statslab.cam.ac.uk/~mcmc/">Repository</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/review.pdf">Neal (1)</a>, <a href="http://projecteuclid.org/download/pdf_1/euclid.aos/1056562461">Neal (2)</a>, <a href="http://www.people.fas.harvard.edu/~junliu/TechRept/99folder/mcmc.pdf">Liu (1)</a>, <a href="http://www.amazon.com/Monte-Carlo-Strategies-Scientific-Computing/dp/0387952306">Liu (2)</a>, <a href="http://www.csss.washington.edu/Papers/wp9.pdf">Besag</a>, <a href="http://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874">Gamerman & Lopes</a>, <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">Murray</a>, <a href="https://www.youtube.com/watch?v=RJjcBwRiR38&index=19&list=PLFHD4aOUZFp3Fx3rfRkBR0XjP1OCcrYXP">M-G</a></i>
                       |
                       |-- NN                      : <a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo Larochelle's tutorial</a>, <a href="https://class.coursera.org/neuralnets-2012-001">Geoffrey Hinton's course</a>, <a href="ftp://ftp.sas.com/pub/neural/FAQ.html">FAQ</a>, <a href="http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html">Bayesian NN FAQ</a>
                       |-- PGM                     : <a href="http://www.bayesnets.com/">BNs Repository</a>
[[ Machine Learning ]] |-- DL                      : <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">book</a>
                       |
                       |-- GP                      : <i><a href="Code/GP_example.m">code</a>, <a href="http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html">covariance function</a>, <a href="http://www.carlboettiger.info/2012/10/17/basic-regression-in-gaussian-processes">GP regression tutorial</a>, <a href="http://www.inference.eng.cam.ac.uk/mng10/GP/">resources</a></i>
                       |-- IBP                     :
                       |-- CRP                     : <i><a href="Code/CRP.m">code</a></i>
                       |-- LDA                     :
                       |-- DP                      :                       
                       |
                       |-- SVM                     : <i><a href="http://www.svms.org/">Repository</a>, <a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">Burges' SVM tutorial</a></i>
                       |-- Kernel Methods          : <i><a href="http://www.kernel-machines.org/">Kernel</a></i>                       
                       |-- Deep Learning           : <i><a href="http://deeplearning.net/">Repository</a>              
                       |-- Kernel Embedding        : <i><a href="http://www.cc.gatech.edu/~lsong/papers/lesong_thesis.pdf">Le Song's thesis</a></i>
                       |-- Bayesian Nonparametrics : <i><a href="http://stat.columbia.edu/~porbanz/reports/porbanz_BNP_draft.pdf">review</a></i>

- <a href="http://alumni.media.mit.edu/~tpminka/statlearn/glossary/">Glossary</a>, <a href="http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlbks.htm">Resources</a>, <a href="http://scikit-learn.org/">scikit</a>, <a href="https://probmods.org/">Church</a>
- A blog post on CRP, DP, and more: <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Link</a>
- Research Blogs: <a href="http://www.37steps.com">37steps</a>
- AMA: <a href="http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun">Yann Lecun</a></i>, <i><a href="http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio">Yoshua Bengio</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/">Michael Jordan</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/">Andrew Ng</a></i>
- Tutorials: <a href="http://www.autonlab.org/tutorials/index.html">Tutorial Slides by Andrew Moore</a>


### Notes (Password Protected) ###
- Single Layer Perceptron
  - identity activation function -> linear regression
  - logistic/sigmoid activation function -> logistic regression
- Nuerual Networks (Feedforward, Recurrent, Convolutional, Restricted Boltzmann Machine, Self Organizing Map, Autoencoder)
>> Gaussian Distribution : <a href="Notes/Machine_Learning/Gaussian/Gaussian.pdf">Guassian Distribution</a>
>> Dirichlet Distribution : <a href="Notes/Machine_Learning/DiricheletDistribution/DiricheletDistribution.pdf">Dirichlet Distribution</a>
>> algebra for random variables : sum distribution, difference distribution, product distribution, ratio distribution (TBD)
>> Matrix Identities (TBD)
>> Convex Optimization (TBD)
>> Numerical Optimization (TBD)
>> Classic Books (Bishop + Murphy + Barber + Hastie/Tibshirani/Friedman)
>> MCMC

Textbook:
== Machine Learning ==
- Bishop, <i>Pattern Recognition and Machine Learning</i>
- HTF, <i>The Elements of Statistical Learning</i>
     - <a href="http://waxworksmath.com/Authors/G_M/Hastie/hastie.html">Solutions</a>
- Murphy, <i>Machine Learning</i>

== Optimization ==
- Boyd, <i>Convex Optimization</i>
- NW, <i>Numerical Optimization</i>

Reference:
1) MacKay, <i>Information Theory, Inference, and Learning Algorithms</i>


== Deep Learning ==
- <a href="http://neuralnetworksanddeeplearning.com/">Tutorial1</a>, <a href="http://deeplearning.net/tutorial/">Tutorial2</a>, <a href="http://ufldl.stanford.edu/tutorial/">Tutorial3</a>
- <a href="http://karpathy.github.io/neuralnets/">Blog1</a>, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Blog2</a>, <a href="http://karpathy.github.io/2015/10/25/selfie/">Blog3</a>
- <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Paper1</a>, <a href="http://arxiv.org/pdf/1206.5538v3.pdf">Paper2</a>, <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Paper3</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Paper4</a>

- <a href="http://deeplearning.net/">Website</a>, <a href="http://www.deeplearningbook.org/">Book</a>

and <a href="http://www.liaoxuefeng.com">GIT</a>


== Good Readings ==
$$ MCMC $$
- Radford M. Neal's work of MCMC on DP (<a href="http://www.cs.toronto.edu/~radford/ftp/bmm.pdf">1991</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf">1998</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit.pdf">2000</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit2.pdf">2005</a>).
- <i>Collapsed Gibbs Sampler</i>. Jun S. Liu. (<a href="http://www.people.fas.harvard.edu/~junliu/TechRept/94folder/collaps2.pdf">Link</a>)

$$ Kernel Methods $$
- <i>RKHS tutorial</i>. Hal Daum`e III. (<a href="http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf">Link</a>)

$$ Variational Methods $$
- <i>From BP to EP</i>. Kevin P Murphy. (<a href="http://www.cs.ubc.ca/~murphyk/Papers/EP.ps.gz">Link</a>)

$$ Tutorial $$
- <i>Conjugate analysis of Gaussian</i>. Kevin P Murphy. (<a href="http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf">Link</a>)


== Glossary ==
- Model Evidence: Model evidence, or marginal likelihood function, is a likelihood function in which some parameter variables have been marginalized, usually denoted by P(D|M).
                  For example, in polynomial regression, M may denotes the degree of the regression function, and parameter variables are the coefficients for any given M.



### FAQ ###
- Q: Why is the name "statistical" machine learning / data mining / pattern recognition?
- A: It means the data is in vector form and not in, for example, strings, where it will be called "syntactical/structural" pattern recognition.

- Q: Unbiased v.s. consistent estimators
- A: Roughly speaking, "unbiased" is related to repeated experiments with the same number of samples while "consistent" is related to increased number of samples.
     Unbiased is "vertical", and consistent is "horizontal". There are examples that are biased but consistent, and unbiased but not consistent.



### Paper List ###
<a href="http://jmlr.csail.mit.edu/proceedings">JMLR</a>
<a href="http://books.nips.cc">NIPS</a>
<a href="http://www.machinelearning.org/icml.html">ICML</a>
<a href="http://uai.sis.pitt.edu/proceedings.jsp?mmnu=1&smnu=0">UAI</a>

<a href="http://machinelearning.wustl.edu/mlpapers/venues">Repository</a>     


<hr> <!-- a horizontal line divider -->

<center>~ Simple and Beautiful ~</center>
<center>[Last Update: 6/14/2016]</center>

</PRE>

</body>

</html>
