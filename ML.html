<meta charset="UTF-8">
<html>
<title> Xi Tan's homepage at Purdue University </title>
<body background="index.files/backmath.gif">

<PRE>

== PRML ==
1. Introduction
2. Probability Distributions
3. Linear Models for Regression
4. Linear Models for Classification
5. Neural Networks
6. Kernel Methods
7. Sparse Kernel Machines
8. Graphical Models
9. Mixture Models and EM
10. Approximate Inference
11. Sampling Methods
12. Continuous Latent Variables
13. Sequential Data
14. Combining Models


== ESL ==
1. Introduction
2. Overview of Supervised Learning
3. Linear Methods for Regression
4. Linear Methods for Classification
5. Basis Expansions and Regularization
6. Kernel Methods
7. Model Assessment and Selection
8. Model Inference and Averaging
9. Additive Models, Trees, and Related Methods
10. Boosting and Additive Trees
11. Neural Networks
12. Support Vector Machines and Flexible Discriminants
13. Prototype Methods and Nearest-Neighbors
14. Unsupervised Learning


== MLPP ==
1 Introduction
2 Probability
3 Generative models for discrete data
4 Gaussian models
5 Bayesian statistics
6 Frequentist statistics
7 Linear regression
8 Logistic regression
9 Generalized linear models and the exponential family
10 Directed graphical models (Bayes nets)
11 Mixture models and the EM algorithm
12 Latent linear models
13 Sparse linear models
14 Kernels
15 Gaussian processes
16 Adaptive basis function models
17 Markov and hidden Markov models
18 State space models
19 Undirected graphical models (Markov random fields)
20 Exact inference for graphical models
21 Variational inference
22 More variational inference
23 Monte Carlo inference
24 Markov chain Monte Carlo (MCMC) inference
25 Clustering
26 Graphical model structure learning
27 Latent variable models for discrete data
28 Deep learning


== BRML ==
{ I Inference in Probabilistic Models }
1 Probabilistic Reasoning
2 Basic Graph Concepts
3 Belief Networks
4 Graphical Models
5 Efficient Inference in Trees
6 The Junction Tree Algorithm
7 Making Decisions

{ II Learning in Probabilistic Models }
8 Statistics for Machine Learning
9 Learning as Inference
10 Naive Bayes
11 Learning with Hidden Variables
12 Bayesian Model Selection

{ III Machine Learning }
13 Machine Learning Concepts
14 Nearest Neighbor Classification
15 Unsupervised Linear Dimension Reduction
16 Supervised Linear Dimension Reduction
17 Linear Models
18 Bayesian Linear Models
19 Gaussian Processes
20 Mixture Models
21 Latent Linear Models
22 Latent Ability Models

{ IV Dynamical Models }
23 Discrete-State Markov Models
24 Continuous-state Markov Models
25 Switching Linear Dynamical Systems
26 Distributed Computation
{ V Approximate Inference }
27 Sampling
28 Deterministic Approximate Inference


<hr> <!-- a horizontal line divider -->


== Convex Optimization [<a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info">courseware</a>] ==
1 Introduction
2 Convex sets
3 Convex functions
4 Convex optimization problems
5 Duality
6 Approximation and fitting
7 Statistical estimation
8 Geometric problems
9 Unconstrained minimization
10 Equality constrained minimization
11 Interior-point methods


== Numerical Optimization ==
1 Introduction
2 Fundamentals of Unconstrained Optimization
3 Line Search Methods
4 Trust-Region Methods
5 Conjugate Gradient Methods
6 Quasi-Newton Methods
7 Large-Scale Unconstrained Optimization
8 Calculating Derivatives
9 Derivative-Free Optimization
10 Least-Squares Problems
11 Nonlinear Equations
12 Theory of Constrained Optimization
13 Linear Programming: The Simplex Method
14 Linear Programming: Interior-Point Methods
15 Fundamentals of Algorithms for Nonlinear Constrained Optimization
16 Quadratic Programming
17 Penalty and Augmented Lagrangian Methods
18 Sequential Quadratic Programming
19 Interior-Point Methods for Nonlinear Programming

<hr> <!-- a horizontal line divider -->


### Canada ###
- U. of Montreal          : Yoshua Bengio (<i><u>Deep Learning</i></u>)
- U. of Toronto           : Geoffrey Hinton (<i><u>Deep Learning</i></u>), Craig Boutilier, Brendan Frey, Radford Neal (<i><u>MCMC</i></u>)
- U. of Alberta           : Dale Schuurmans

### Europe ###
- U. of Cambridge         : Zoubin Ghahramani, David MacKay (<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video</a>), Carl Rasmussen, José Miguel Hernández-Lobato
- U. of Oxford            : Nando DeFrietas, Steffen Lauritzen
- U. of Sheffield         : Neil Lawrence
- U. of Bristol           : Nello Cristianini
- U. of London            : John Shawe-Taylor (<i><u>SVM, Kernel</i></u>), Vladimir Vapnik (<i><u>SVM</i></u>)
- ETH                     : Joachim Buhman
- Max Planck Institute    : Bernhard Schoelkopf, Michael Black
- Hebrew U. of Jerusalem  : Nir Friedman, Yair Weiss
- Microsoft Research (UK) : Christopher Bishop, Tom Minka (<i><u>EP</i></u>), Andrew Blake
- University of Geneva    : Lorenzo Rosasco

### USA ###
== East ==
- UMD                     : Lise Getoor
- Brown                   : Thomas Hoffman, Michael Littman
- Rutgers                 : Tong Zhang, Vladimir Pavlovic
- U. of Mass              : Andrew McCallum
- U. of Penn              : Michael Kearns, Jianbo Shi
- NYU                     : Yann LeCun (<i><u>Deep Learning</i></u>), Sam Roweis, Mehryar Mohri
- Harvard                 : Avi Pfeffer
- Duke                    : Sayan Mukherjee
- CMU                     : Andrew Moore, Tom Mitchell, Alex Smola (Kernel), John Lafferty
- MIT                     : Leslie Kaelbling, Tommi Jaakkola, Bill Freeman, Alex Pentland, Josh Tenenbaum
- Georgia Tech            : Aaron Bobick, Le Song (<i><u>Kernel</i></u>), Irfan Essa, Charles Isbell
- Princeton               : David Blei (<i><u>LDA</i></u>), Rob Schapire
- Cornell                 : Thorsten Joachims, Rich Caruana
- Columbia                : Tony Jebara, Michael Collins, Rocco Servedio, Peter Orbanz

== West ==
- Microsoft Research (US) : David Heckerman, Eric Horovitz, Nebojsa Jojic, Phil Long, Chris Meek, John Platt, Léon Bottou
- Google Research         : Kevin Murphy, Yoram Singer, Samy Bengio, Corinna Cortes, Fernando Perreira
- Stanford                : Andrew Ng (<i><u>Deep Learning</i></u>), Daphne Koller (<i><u>PGM</i></u>)
- UC-Berkeley             : Michael Jordan (<i><u>PGM</i></u>), Peter Bartlett, Leo Breiman, Martin Wainwright
- UW-Seattle              : Marina Meila, Jeff Bilmes, Dieter Fox, Raj Rao
- UCLA                    : Judea Pearl (<i><u>PGM/Causal</u></i>)
- UCI                     : Padhraic Smyth
- UCSC                    : Manfred Warmuth
- UCSD                    : Yoav Freund, Lawrence Saul
- Oregon State U.         : Tom Dietterich

== Elsewhere ==
- Wisconsin               : Xiaojin Zhu
- U. of Chicago           : Partha Niyogi
- UIUC                    : David Forsyth
- U. of Texas             : Adam Klivans
</div>

<hr> <!-- a horizontal line divider -->
                       |-- Introduction            : <a href="https://work.caltech.edu/telecourse.html">Caltech</a>, <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/">AI course</a>
                       |
                       |-- Linear Programming      : <a href="https://class.coursera.org/linearprogramming-001">Coursera</a>                       
                       |
                       |-- VB                      : <i><a href="http://www.variational-bayes.org">Repository</a></i>
                       |-- EP                      : <i><a href="http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html">Roadmap</a></i>                       
                       |-- MCMC                    : <i><a href="http://onionesquereality.wordpress.com/2008/08/31/demystifying-support-vector-machines-for-beginners/">Blog</a>, <a href="http://www.statslab.cam.ac.uk/~mcmc/">Repository</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/review.pdf">Neal</a>, <a href="http://www.people.fas.harvard.edu/~junliu/TechRept/99folder/mcmc.pdf">Liu (1)</a>, <a href="http://www.amazon.com/Monte-Carlo-Strategies-Scientific-Computing/dp/0387952306">Liu (2)</a>, <a href="http://www.csss.washington.edu/Papers/wp9.pdf">Besag</a>, <a href="http://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874">Gamerman & Lopes</a>, <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">Murray</a>, <a href="https://www.youtube.com/watch?v=RJjcBwRiR38&index=19&list=PLFHD4aOUZFp3Fx3rfRkBR0XjP1OCcrYXP">M-G</a></i>
                       |
                       |-- NN                      : <a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo Larochelle's tutorial</a>, <a href="https://class.coursera.org/neuralnets-2012-001">Geoffrey Hinton's course</a>, <a href="ftp://ftp.sas.com/pub/neural/FAQ.html">FAQ</a>, <a href="http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html">Bayesian NN FAQ</a>
                       |-- PGM                     : <a href="http://www.bayesnets.com/">BNs Repository</a>
[[ Machine Learning ]] |-- DL                      : <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">book</a>
                       |
                       |-- GP                      : <i><a href="Code/GP_example.m">code</a>, <a href="http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html">covariance function</a>, <a href="http://www.carlboettiger.info/2012/10/17/basic-regression-in-gaussian-processes">GP regression tutorial</a>, <a href="http://www.inference.eng.cam.ac.uk/mng10/GP/">resources</a></i>                                          
                       |-- CRP                     : <i><a href="Code/CRP.m">code</a></i>                       
                       |
                       |-- SVM                     : <i><a href="http://www.svms.org/">Repository</a>, <a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">Burges' SVM tutorial</a></i>
                       |-- Kernel Methods          : <i><a href="http://www.kernel-machines.org/">Kernel</a>, <a href="http://alex.smola.org/papers/2003/SchSmo03c.pdf">Introduction</a></i>                       
                       |-- Deep Learning           : <i><a href="http://deeplearning.net/">Repository</a>, <a href="http://neuralnetworksanddeeplearning.com/">Tutorial1</a>, <a href="http://deeplearning.net/tutorial/">Tutorial2</a>, <a href="http://ufldl.stanford.edu/tutorial/">Tutorial3</a>, <a href="http://karpathy.github.io/neuralnets/">Blog1</a>, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Blog2</a>, <a href="http://karpathy.github.io/2015/10/25/selfie/">Blog3</a>, <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Paper1</a>, <a href="http://arxiv.org/pdf/1206.5538v3.pdf">Paper2</a>, <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Paper3</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Paper4</a>, <a href="http://deeplearning.net/">Website</a>, <a href="http://www.deeplearningbook.org/">Book</a></i>                    


- <a href="http://alumni.media.mit.edu/~tpminka/statlearn/glossary/">Glossary</a>, <a href="http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlbks.htm">Resources</a>, <a href="http://scikit-learn.org/">scikit</a>, <a href="https://probmods.org/">Church</a>
- A blog post on CRP, DP, and more: <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Link</a>
- Research Blogs: <a href="http://www.37steps.com">37steps</a>
- AMA: <a href="http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun">Yann Lecun</a></i>, <i><a href="http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio">Yoshua Bengio</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/">Michael Jordan</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/">Andrew Ng</a></i>
- Tutorials: <a href="http://www.autonlab.org/tutorials/index.html">Tutorial Slides by Andrew Moore</a>



Readings:
- Causal inference in statistics: An overview <a href="http://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf">link</a>


== Good Readings ==
$$ MCMC $$
- Radford M. Neal's work of MCMC on DP (<a href="http://www.cs.toronto.edu/~radford/ftp/bmm.pdf">1991</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf">1998</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit.pdf">2000</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit2.pdf">2005</a>).
- <i>Collapsed Gibbs Sampler</i>. Jun S. Liu. (<a href="http://www.people.fas.harvard.edu/~junliu/TechRept/94folder/collaps2.pdf">Link</a>)

$$ Kernel Methods $$
- <i>RKHS tutorial</i>. Hal Daum`e III. (<a href="http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf">Link</a>)

$$ Variational Methods $$
- <i>From BP to EP</i>. Kevin P Murphy. (<a href="http://www.cs.ubc.ca/~murphyk/Papers/EP.ps.gz">Link</a>)


== Notes ==
>> Gaussian Distribution : <a href="Notes/Machine_Learning/Gaussian/Gaussian.pdf">Guassian Distribution</a>, <a href="http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf">Conjugate analysis of Gaussian</a>
>> Dirichlet Distribution : <a href="Notes/Machine_Learning/DiricheletDistribution/DiricheletDistribution.pdf">Dirichlet Distribution</a>


== Glossary ==
- Model Evidence: Model evidence, or marginal likelihood function, is a likelihood function in which some parameter variables have been marginalized, usually denoted by P(D|M).
                  For example, in polynomial regression, M may denotes the degree of the regression function, and parameter variables are the coefficients for any given M.



### FAQ ###
- Q: Why is the name "statistical" machine learning / data mining / pattern recognition?
- A: It means the data is in vector form and not in, for example, strings, where it will be called "syntactical/structural" pattern recognition.

- Q: Unbiased v.s. consistent estimators
- A: Roughly speaking, "unbiased" is related to repeated experiments with the same number of samples while "consistent" is related to increased number of samples.
     Unbiased is "vertical", and consistent is "horizontal". There are examples that are biased but consistent, and unbiased but not consistent.



### Paper List ###
<a href="http://jmlr.csail.mit.edu/proceedings">JMLR</a>
<a href="http://books.nips.cc">NIPS</a>
<a href="http://www.machinelearning.org/icml.html">ICML</a>
<a href="http://uai.sis.pitt.edu/proceedings.jsp?mmnu=1&smnu=0">UAI</a>

<a href="http://machinelearning.wustl.edu/mlpapers/venues">Repository</a>     


<hr> <!-- a horizontal line divider -->

<center>~ Simple and Beautiful ~</center>
<center>[Last Update: 6/18/2016]</center>

</PRE>

</body>

</html>
