<meta charset="UTF-8">
<html>
<title> Xi Tan's homepage at Purdue University </title>
<body background="index.files/backmath.gif">

<PRE>
Some good free video lectures:
- CMU ML (10-701): <a href="http://alex.smola.org/teaching/cmu2013-10-701/">F13</a>
- CMU ML (10-702): <a href="http://www.stat.cmu.edu/~larry/=sml/">S16</a>
- CMU PGM (10-708): <a href="http://www.cs.cmu.edu/~epxing/Class/10708/lecture.html">S14</a>
- CMU Optimization (10-725): <a href="https://www.cs.cmu.edu/~ggordon/10725-F12/schedule.html">F12</a>
- Stanford CNN (cs231n): <a href="http://cs231n.stanford.edu/">S16</a>
- UCL Reinforcement Learning: <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">RL</a>

= Main Reference =
1. Pattern Recognition and Machine Learning (<b>PRML</b>) [<a href="http://www.springer.com/cda/content/document/cda_downloaddocument/9780387310732-t1.pdf">toc</a>]
   1) Introduction
   2) Probability Distributions
2. Machine Learning: A Probabilistic Perspective (<b>MLPP</b>) [<a href="https://www.cs.ubc.ca/~murphyk/MLbook/pml-toc-22may12.pdf">toc</a>]
   1) Introduction
   2) Probability
   3) Generative models for discrete data
   4) Gaussian models
   5) Bayesian statistics
   6) Frequentist statistics
3. Bayesian Reasoning and Machine Learning (<b>BRML</b>) [<a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/181115.pdf">Ebook</a>]
   1) Probabilistic reasoning
   8) Statistics for machine learning
   13) Machine learning concepts
4. Machine Learning: A Bayesian and Optimization Perspective (<b>MLBOP</b>)
   1) Introduction
   2) Probability and stochastic processes
   3) Learning in parametric modeling: basic concepts and directions
5. Pattern Recognition (<b>PR</b>)
   1) Introduction   
6. Pattern Classification (<b>PC</b>)
   1) Introduction
   2) Bayesian decision theory
   3) Maximum-likelihood and Bayesian parameter estimation
7. The Elements of Statistical Learning (<b>ESL</b>) [<a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/contents.pdf">toc</a>, <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf">Ebook</a>]
   1) Introduction
   2) Overview of supervised learning
8. Bayesian Data Analysis (<b>BDA</b>)
   1) Probability and inference
   2) Single-parameter models
   3) Introduction to multi-parameter models
   4) Asymptotic and connections to non-Bayesian approaches
   5) Hierarchical models
   6) Model checking
   7) Evaluating, comparing, and expanding models
   8) Modeling accounting for data collection
   9) Decision analysis


9. Introduction to Linear Optimization
   1) Introduction
   2) The geometry of linear programming
10. Nonlinear Programming
   1) Unconstrained Optimization: Basic Methods
   2) Unconstrained Optimization: Additional Methods
11. Convex Optimization [<a href="https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info">courseware</a>, <a href="Notes/Convex Optimization Notes/ConvexOptimizationNotes.pdf">Notes</a>]
   1) Introduction &#10004; (2016/10/19)
   2) Convex sets
   3) Convex functions
   4) Convex optimization problems
   5) Duality
12. Numerical Optimization
   1) Introduction
   2) Fundamentals of Unconstrained Optimization
   3) Line search methods

<hr> <!-- a horizontal line divider -->


### Canada ###
- U. of Montreal          : Yoshua Bengio (<i><u>Deep Learning</i></u>)
- U. of Toronto           : Geoffrey Hinton (<i><u>Deep Learning</i></u>), Craig Boutilier, Brendan Frey, Radford Neal (<i><u>MCMC</i></u>)
- U. of Alberta           : Dale Schuurmans

### Europe ###
- U. of Cambridge         : Zoubin Ghahramani, David MacKay (<a href="http://videolectures.net/course_information_theory_pattern_recognition/">video</a>), Carl Rasmussen, José Miguel Hernández-Lobato
- U. of Oxford            : Nando DeFrietas, Steffen Lauritzen
- U. of Sheffield         : Neil Lawrence
- U. of Bristol           : Nello Cristianini
- U. of London            : John Shawe-Taylor (<i><u>SVM, Kernel</i></u>), Vladimir Vapnik (<i><u>SVM</i></u>)
- ETH                     : Joachim Buhman
- Max Planck Institute    : Bernhard Schoelkopf, Michael Black
- Hebrew U. of Jerusalem  : Nir Friedman, Yair Weiss
- Microsoft Research (UK) : Christopher Bishop, Tom Minka (<i><u>EP</i></u>), Andrew Blake
- University of Geneva    : Lorenzo Rosasco

### USA ###
== East ==
- UMD                     : Lise Getoor
- Brown                   : Thomas Hoffman, Michael Littman
- Rutgers                 : Tong Zhang, Vladimir Pavlovic
- U. of Mass              : Andrew McCallum
- U. of Penn              : Michael Kearns, Jianbo Shi
- NYU                     : Yann LeCun (<i><u>Deep Learning</i></u>), Sam Roweis, Mehryar Mohri
- Harvard                 : Avi Pfeffer
- Duke                    : Sayan Mukherjee
- CMU                     : Andrew Moore, Tom Mitchell, Alex Smola (Kernel), John Lafferty
- MIT                     : Leslie Kaelbling, Tommi Jaakkola, Bill Freeman, Alex Pentland, Josh Tenenbaum
- Georgia Tech            : Aaron Bobick, Le Song (<i><u>Kernel</i></u>), Irfan Essa, Charles Isbell
- Princeton               : David Blei (<i><u>Topic Models</i></u>), Rob Schapire
- Cornell                 : Thorsten Joachims, Rich Caruana
- Columbia                : Tony Jebara, Michael Collins, Rocco Servedio, Peter Orbanz

== West ==
- Microsoft Research (US) : David Heckerman, Eric Horovitz, Nebojsa Jojic, Phil Long, Chris Meek, John Platt, Léon Bottou
- Google Research         : Kevin Murphy, Yoram Singer, Samy Bengio, Corinna Cortes, Fernando Perreira
- Stanford                : Andrew Ng (<i><u>Deep Learning</i></u>), Daphne Koller (<i><u>PGM</i></u>)
- UC-Berkeley             : Michael Jordan (<i><u>PGM</i></u>), Peter Bartlett, Leo Breiman, Martin Wainwright
- UW-Seattle              : Marina Meila, Jeff Bilmes, Dieter Fox, Raj Rao
- UCLA                    : Judea Pearl (<i><u>PGM/Causal</u></i>)
- UCI                     : Padhraic Smyth
- UCSC                    : Manfred Warmuth
- UCSD                    : Yoav Freund, Lawrence Saul
- Oregon State U.         : Tom Dietterich

== Elsewhere ==
- Wisconsin               : Xiaojin Zhu
- U. of Chicago           : Partha Niyogi
- UIUC                    : David Forsyth
- U. of Texas             : Adam Klivans
</div>

<hr> <!-- a horizontal line divider -->
                       |-- Introduction            : <a href="https://work.caltech.edu/telecourse.html">Caltech</a>, <a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/">AI course</a>
                       |
                       |-- Linear Programming      : <a href="https://class.coursera.org/linearprogramming-001">Coursera</a>                       
                       |
                       |-- VB                      : <i><a href="http://www.variational-bayes.org">Repository</a></i>
                       |-- EP                      : <i><a href="http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html">Roadmap</a></i>                       
                       |-- MCMC                    : <i><a href="http://onionesquereality.wordpress.com/2008/08/31/demystifying-support-vector-machines-for-beginners/">Blog</a>, <a href="http://www.statslab.cam.ac.uk/~mcmc/">Repository</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/review.pdf">Neal</a>, <a href="http://www.people.fas.harvard.edu/~junliu/TechRept/99folder/mcmc.pdf">Liu (1)</a>, <a href="http://www.amazon.com/Monte-Carlo-Strategies-Scientific-Computing/dp/0387952306">Liu (2)</a>, <a href="http://www.csss.washington.edu/Papers/wp9.pdf">Besag</a>, <a href="http://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874">Gamerman & Lopes</a>, <a href="http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf">Murray</a>, <a href="https://www.youtube.com/watch?v=RJjcBwRiR38&index=19&list=PLFHD4aOUZFp3Fx3rfRkBR0XjP1OCcrYXP">M-G</a></i>
                       |
                       |-- NN                      : <a href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo Larochelle's tutorial</a>, <a href="https://class.coursera.org/neuralnets-2012-001">Geoffrey Hinton's course</a>, <a href="ftp://ftp.sas.com/pub/neural/FAQ.html">FAQ</a>, <a href="http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html">Bayesian NN FAQ</a>
                       |-- PGM                     : <a href="http://www.bayesnets.com/">BNs Repository</a>
[[ Machine Learning ]] |-- DL                      : <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">book</a>
                       |
                       |-- GP                      : <i><a href="Code/GP_example.m">code</a>, <a href="http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html">covariance function</a>, <a href="http://www.carlboettiger.info/2012/10/17/basic-regression-in-gaussian-processes">GP regression tutorial</a>, <a href="http://www.inference.eng.cam.ac.uk/mng10/GP/">resources</a></i>                                          
                       |-- CRP                     : <i><a href="Code/CRP.m">code</a></i>                       
                       |
                       |-- SVM                     : <i><a href="http://www.svms.org/">Repository</a>, <a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">Burges' SVM tutorial</a></i>
                       |-- Kernel Methods          : <i><a href="http://www.kernel-machines.org/">Kernel</a>, <a href="http://alex.smola.org/papers/2003/SchSmo03c.pdf">Introduction</a></i>                       
                       |-- Deep Learning           : <i><a href="http://deeplearning.net/">Repository</a>, <a href="http://neuralnetworksanddeeplearning.com/">Tutorial1</a>, <a href="http://deeplearning.net/tutorial/">Tutorial2</a>, <a href="http://ufldl.stanford.edu/tutorial/">Tutorial3</a>, <a href="http://karpathy.github.io/neuralnets/">Blog1</a>, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Blog2</a>, <a href="http://karpathy.github.io/2015/10/25/selfie/">Blog3</a>, <a href="http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf">Paper1</a>, <a href="http://arxiv.org/pdf/1206.5538v3.pdf">Paper2</a>, <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Paper3</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Paper4</a>, <a href="http://deeplearning.net/">Website</a>, <a href="http://www.deeplearningbook.org/">Book</a></i>                    


- <a href="http://alumni.media.mit.edu/~tpminka/statlearn/glossary/">Glossary</a>, <a href="http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlbks.htm">Resources</a>, <a href="http://scikit-learn.org/">scikit</a>, <a href="https://probmods.org/">Church</a>
- A blog post on CRP, DP, and more: <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Link</a>
- Research Blogs: <a href="http://www.37steps.com">37steps</a>
- AMA: <a href="http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun">Yann Lecun</a></i>, <i><a href="http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio">Yoshua Bengio</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/">Michael Jordan</a></i>, <i><a href="https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/">Andrew Ng</a></i>
- Tutorials: <a href="http://www.autonlab.org/tutorials/index.html">Tutorial Slides by Andrew Moore</a>



Readings:
- Causal inference in statistics: An overview <a href="http://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf">link</a>


== Good Readings ==
$$ MCMC $$
- Radford M. Neal's work of MCMC on DP (<a href="http://www.cs.toronto.edu/~radford/ftp/bmm.pdf">1991</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf">1998</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit.pdf">2000</a>, <a href="http://www.cs.toronto.edu/~radford/ftp/mixsplit2.pdf">2005</a>).
- <i>Collapsed Gibbs Sampler</i>. Jun S. Liu. (<a href="http://www.people.fas.harvard.edu/~junliu/TechRept/94folder/collaps2.pdf">Link</a>)

$$ Kernel Methods $$
- <i>RKHS tutorial</i>. Hal Daum`e III. (<a href="http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf">Link</a>)

$$ Variational Methods $$
- <i>From BP to EP</i>. Kevin P Murphy. (<a href="http://www.cs.ubc.ca/~murphyk/Papers/EP.ps.gz">Link</a>)


== Notes ==
>> Gaussian Distribution : <a href="Notes/Machine_Learning/Gaussian/Gaussian.pdf">Guassian Distribution</a>, <a href="http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf">Conjugate analysis of Gaussian</a>
>> Dirichlet Distribution : <a href="Notes/Machine_Learning/DiricheletDistribution/DiricheletDistribution.pdf">Dirichlet Distribution</a>


== Glossary ==
- Model Evidence: Model evidence, or marginal likelihood function, is a likelihood function in which some parameter variables have been marginalized, usually denoted by P(D|M).
                  For example, in polynomial regression, M may denotes the degree of the regression function, and parameter variables are the coefficients for any given M.



### FAQ ###
- Q: Why is the name "statistical" machine learning / data mining / pattern recognition?
- A: It means the data is in vector form and not in, for example, strings, where it will be called "syntactical/structural" pattern recognition.

- Q: Unbiased v.s. consistent estimators
- A: Roughly speaking, "unbiased" is related to repeated experiments with the same number of samples while "consistent" is related to increased number of samples.
     Unbiased is "vertical", and consistent is "horizontal". There are examples that are biased but consistent, and unbiased but not consistent.

- Q: How can I categorize machine learning models and methods?
- A: In general, machine learning models can be categorized according to their strategies for generating features:
1) fixed basis function models - basis functions are pre-designed;
2) adaptive basis function models (CART, Neural Networks) - form or parameters of basis functions learned from data;
3) kernel models (SVM, GP) - basis functions are implicitly defined, dimension of which is essentially infinite;
4) latent variable models / dimension reduction (HMMs, Sate Space Models; PCA, ICA).

As to machine learning inference methods, they can be classified into three categories:
1) exact inference / direct inference (conjugacy, convex optimization, numerical optimization);
2) deterministic approximation inference (Variational Bayes and Expectation Propagation);
3) stochastic approximation inference (MCMC).

Ask the following questions, in the listed order, for each machine learning problem:
1) task: regression / classification / clustering
2) data: supervised learning / unsupervised learning
3) feature generation: fixed / adaptive / kernel / latent
4) model: parametric / nonparametric
5) inference: exact / variational / MCMC



### Paper List ###
<a href="http://jmlr.csail.mit.edu/proceedings">JMLR</a>
<a href="http://books.nips.cc">NIPS</a>
<a href="http://www.machinelearning.org/icml.html">ICML</a>
<a href="http://uai.sis.pitt.edu/proceedings.jsp?mmnu=1&smnu=0">UAI</a>

<a href="http://machinelearning.wustl.edu/mlpapers/venues">Repository</a>     


<hr> <!-- a horizontal line divider -->

<center>~ Simple and Beautiful ~</center>
<center>[Last Update: 9/17/2016]</center>

</PRE>

</body>

</html>
