# jemdoc: menu{MENU_NEW}{index.html}
== What's New

~~~
2\/25\/2018 0531 Sunday Sunny
~~~

在$LaTeX$的双行环境下，$figure*$和$table*$可以让图表跨栏。

~~~
1\/15\/2018 1235 Monday Snowy
~~~

"我以为人的生活，可以分作三层：一是物质生活，二是精神生活，三是灵魂生活。物质生活就是衣食。精神生活就是学术文艺。灵魂生活就是宗教。" —— 丰子恺。

~~~
1\/10\/2018 0515 Wednesday Sunny
~~~

*How to type French accents*

Ref: http://french.about.com/od/writing/ss/typeaccents_8.htm

______________

To type accents on an Apple with the option key, hold down the option key while pressing the key(s) in bold in this list. For example, to type ê, hold the option key while typing i, then release both and type e. To type î, hold option, type i, release and type i again.

Note: In these instructions, "and" means to keep holding the option key and the first key listed while typing the second. "Then" means to release the option key and the first key before typing the second.

acute accent
é   Hold option key and e then e

grave accent
à, è, ù   Hold option key and ` then a, e, or u

cedilla
ç   Hold option key and c

circumflex
â, ê, î, ô, û   Hold option key and i then a, e, i, o, or u

tréma
ë, ï, ü   Hold option key and u then e, i, or u

oe ligature
œ   Hold option key and q

French quotation marks
«   Hold option key and \
»   Hold option key and shift key and \

Euro symbol
€   Hold option key and shift key and 2


~~~
1\/10\/2018 0515 Wednesday Sunny
~~~

*Geometry of a Hyperplan*

A hyperplane is a set satisfies $H=\{x:w^Tx=b\}$. An equivalent form is $w^T(x-\frac{w}{\|w\|^2}b) = 0$, which suggests that the vector $w$ is perpendicular to the hyperplane, called a normal vector.

Particularly, since $\frac{w^Tw}{\|w\|^2}b = b$, we know $x_0=\frac{w}{\|w\|}\frac{b}{\|w\|}$ is on the hyperplane. The $x_0$ is actually the projection of the origin, since $w$ is orthogonal to the hyperplane and it is nothing but a scaled $w$ on the hyperplane. Therefore, the shortest distance (along the direction of $w$) from the origin to the hyperplane is given by $\frac{b}{\|w\|}$ (could be negative, which means $w$ is on the other side of the hyperplane).

In general, if a hyperplane is given by the equation $f(x) = w^Tx-b = 0$, the distance from any arbitrary vector $p$ to the hyperplane $w^Tx=b$ is given by

        $\frac{f(p)}{\|w\|} = \frac{w^Tp-b}{\|w\|}$,           if $p$ is on the opposite side of the origin

$-\frac{f(p)}{\|w\|} = -\frac{w^Tp-b}{\|w\|}$,         if $p$ is on the same side of the origin

Particularly, when $p=0$ (the origin), the above becomes $\frac{b}{\|w\|}$, which agrees with our previous result.

Proof: Let's prove the first case. Suppose there is a vector $x$ on the hyperplane, such that $p-x = d\frac{w}{\|w\|}$. Since $w$ is orthogonal to the hyperplane, the scalar $d$ is the distance we are after. Now, multiply both sides by $w^T$,

$w^Tp-w^Tx = dw^T\frac{w}{\|w\|}$

$w^Tp-(w^Tx-b) = d\frac{w^Tw}{\|w\|}+b$

$w^Tp = d\|w\| + b$

$d = \frac{w^Tp-b}{\|w\|}$

The proof for the other case is similar. $\blacksquare$


~~~
12\/10\/2017 0600 Sunday Sunny
~~~

看了几节菲赫金哥尔茨的《微积分学教程》。戴德金分割（Dedekind Cut）定义无理数。

{Calculus $\implies$ Advanced Calculus $\implies$ Real Analysis | Complex Analysis $\implies$ Functional Analysis} + {Linear Algebra | Abstract Algebra} $\implies$ Optimization $\implies$ Machine Learning


~~~
11\/26\/2017 2345 Sunday Sunny
~~~

Traditional parametric models using a fixed and finite number of parameters can suffer from over- or under-fitting of data when there is a misfit between the complexity of the model (often expressed in terms of the number of parameters) and the amount of data available. As a result, model selection, or the choice of a model with the right complexity, is often an important issue in parametric modeling. Unfortunately, model selection is an operation that is fraught with difficulties, whether we use cross validation or marginal probabilities as the basis for selection.

The Bayesian nonparametric approach is an alternative to parametric modeling and selection. By using a model with an unbounded complexity, underfitting is mitigated, while the Bayesian approach of computing or approximating the full posterior over parameters mitigates over-fitting.

[https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf]

To sum up, in parametric models, model selection is performed through cross-validation or model evidence (aka marginal likelihood); in non-parametric models, it is through Bayesian priors, which is equivalent to a penalization term.


~~~
6\/23\/2017 19：45 星期五 阴
~~~

古希腊建筑
古罗马建筑
拜占庭建筑
罗马式建筑
哥特式建筑
文艺复兴建筑 -> (巴洛克建筑; 古典主义建筑 -> 洛可可建筑)



~~~
6\/22\/2017 23:45 星期四 晴
~~~

Some $LaTeX$ Notes.
- try "\\tfrac" and "\\displaystyle";
- try "\\frac 12";
- "\\, \\; \\space \\quad \\qquad"
- "\\eqref""
- "\\colon"

~~~
6\/1\/2017 07:00 星期四 多云
~~~

再写个双线性函数和二次型？

{{
<iframe src="Meeting.html" width="1200" height="500" frameborder="0" scrolling="yes"></iframe>
}}


~~~
6\/1\/2017 06:30 星期四 多云
~~~

试一下$LaTeX$.

*Definition:* Rectangular matrices $A_{m \times n}$ and $B_{m \times n}$ are said to be /*equivalent*/ if there exist two invertible matrices $P$ and $Q$ such that $B = PAQ$.

*Definition:* Square matrices $A_{n \times n}$ and $B_{n \times n}$ are said to be /*congruent*/ if there exist an invertible matrix $P$ such that $B = P^TAP$.


*Definition:* Square matrices $A_{n \times n}$ and $B_{n \times n}$ are said to be /*similar*/ if there exists an invertible matrix $P$ such that $B = P^{-1}AP$.

Equivalent $\implies$ Congruent $\implies$ Similar

~~~
6\/1\/2017 05:30 星期四 多云
~~~

终于把网站搭好了（jemdoc \+ MathJax \+ pdf2htmlEX）。网站的结构比之前好了很多，而且还支持$\LaTeX$。比如：Inline equation $\int_0^\infty e^x dx$。又比如：Multi-line Equation
\(
\begin{align}
	x &= 2 \\
	y &= \int_0^\infty \log(x) dx \\
\end{align}
\)

再比如：Table
~~~
{}{table}{TABLE}
first entry  | another entry ||
as many rows | as you like   ||
bottom row   | last entry
~~~

{{
<p></p>
<strike>过一阵子我看能不能弄个留言板</strike>。
}}

= Random Page

[http://www.sidneyluo.net/]

[https://arxiv.org/ arXiv]

[https://www.ssrn.com/ SSRN]

[http://cmd.to/fm]

[http://www.matrix67.com]

[http://www.wanmen.org]

交浅言深，君子所戒。

化、生、地

默写世界地图和中国地图

Perfume, Food, Wine, and Tea

园林

-- Ph.D Life: [http://phdcomics.com/ Ph.D Comics], [http://www.pgbovine.net/PhD-memoir.htm The Ph.D Grind], [http://www.cs.unc.edu/~azuma/hitch4.html So Long, and Thanks for the Ph.D], [http://cseweb.ucsd.edu/users/mihir/phd.html The PhD experience] | [http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm How to do research], [http://cs.brown.edu/~sk/Memos/Grad-School-Recos/ How to write a good recommendation letter], [http://users.ecs.soton.ac.uk/hcd/reviewing.html How to review a paper]


>>> My Favorite Introductory Reading List <<<

- Finance (Economics; Derivatives; Pricing Tools)
-- Microeconomic Theory, by Andreu Mas-Colell, Michael D. Whinston, Jerry R. Green.
-- Lectures on Macroeconomics, by Olivier Blanchard, Stanley Fischer.
-- Options, Futures, and Other Derivatives, by John C Hull.
-- Arbitrage Theory in Continuous Time, by Tomas Bjork.
-- Stochastic Calculus for Finance I & II, by Steven Shreve.

- Mathematics (Analysis; Algebra; Geometry; Differential Equations)
-- Calculus, by James Stewart.
-- Principles of Mathematical Analysis, by Walter Rudin.
-- Real Variables, by Alberto Torchinsky.
-- Visual Complex Analysis, by Tristan Needham.
-- Introductory Functional Analysis with Applications, by Erwin Kreyszig.
-- Linear Algebra and Its Applications, by Gilbert Strang.
-- Linear Algebra, by Kenneth M Hoffman, Ray Kunze.
-- Algebra, by Michael Artin.
-- Differential Geometry, by Erwin Kreyszig.
-- Topology Without Tears, by Sidney A. Morris.
-- Ordinary Differential Equations, by Morris Tenenbaum and Harry Pollard.
-- Partial Differential Equations, by Lawrence C. Evans.

- Statistics (Random Variables; Stochastic Processes; Models; Theory; Machine Learning)
-- A First Course in Probability, by Sheldon Ross.
-- Stochastic Processes, by Sheldon Ross.
-- Applied Linear Statistical Models, by Michael H. Kutner, John Neter, Christopher J. Nachtsheim, William Li.
-- Generalized Linear Models, by P. McCullagh and John A. Nelder.
-- Probability: Theory and Examples, by Rick Durrett
-- Statistical Inference, by George Casella and Roger L. Berger.
-- Pattern Recognition and Machine Learning, by Christopher Bishop.
-- Machine Learing, by Kevin Murphy.

- Optimization (Linear; Nonlinear; Convex)
-- Introduction to Linear Optimization, by Dimitris Bertsimas, John N. Tsitsiklis.
-- Nonlinear Programming, by Dimitri Bertsekas.
-- Linear and Nonlinear Optimization, by David G. Luenberger and Yinyu Ye.
-- Numerical Optimization, by Jorge Nocedal, Stephen Wright.
-- Introductory Lectures on Convex Optimization: A Basic Course, by Y. Nesterov.
-- Convex Optimization, by Stephen Boyd, Lieven Vandenberghe.

- Computer Science (System, OS, Network; Algorithms; Coding)
-- Computer Systems: A Programmer's Perspective, by Randal E. Bryant, David R. O'Hallaron.
-- Operating System Concepts, by Abraham Silberschatz, Peter B. Galvin, Greg Gagne.
-- Computer Networking: A Top-Down Approach, by James F. Kurose and Keith W. Ross.
-- Introduction to Algorithms, by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein.
-- C++ Primer, by Stanley B. Lippman, Josée Lajoie, Barbara E. Moo.
-- Thinking in Java, by Bruce Eckel.

