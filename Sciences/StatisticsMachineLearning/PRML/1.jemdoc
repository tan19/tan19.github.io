# jemdoc: menu{MENU_PRML}{1.html}
= Introduction

~~~
*The main goals of this chapter are:*
. Introduce some important concepts through examples.
. Review of probability theory, decision theory, and information theory.

*It consists of the following sections:*
- 1.1 Example: Polynomial Curve Fitting
- 1.2 Probability Theory
- 1.3 Model Selection
- 1.4 The Curse of Dimensionality
- 1.5 Decision Theory
- 1.6 Information Theory
~~~

=== Supervised Learning and Unsupervised Learning
The two major tasks in Machine Learning are *supervised learning* and *unsupervised learning*. In supervised learning, where you have input variables $x_i$ and the tags $t_i$, your tasks are to model (as an approximation) the relationship $f$ between them, i.e., 
\(
\hat t_i = y_i = f(x_i)
\)
These tasks are called classification when $t_i$ and $y_i$ are discrete, and regression when continuous. In unsupervised learning, however, you only have input variables $x_i$ and tags $t_i$ are not available to you. The tasks are usually:
- *clustering*, where your $y_i$ are discrete (a.k.a "clusters");
- *density estimation*, where your $y_i$ are often continuous (more familiar if we write $p(x_i)$ instead);
- *feature learning*, where we only work on $x_i$ themselves and try to discover better representations of them.

=== Frequentist v.s. Bayesian
- From a *Frequentist* perspective, parameters $\bf{w}$ are considered to be fixed but unknown constants, whose values are estimated by some estimators, and error bars around the realized estimate are obtained by considering the distribution of possible data sets (samples), i.e., the estimator itself has a sampling distribution. *Bootstrapping* (re-sample the observed sample with replacement) is a popular method to determining such error bars.
- From a *Bayesian* perspective, there is only one single data set (namely the one that is actually observed), and the parameters $\bf{w}$ are considered to be random variables, the uncertainty of which are expressed through probability distributions over $\bf{w}$.

Bayesian models enjoy the benefits of flexibility and expressiveness, but at the same time suffer from computational bottlenecks. However, thanks to the recent development of computing power, Monte Carlo methods are made possible for a wide range of applications. In addition, the creation of a set of highly efficient deterministic approximation algorithms such as VB and EP, offers an alternative to the sampling methods.

=== Generative Models v.s. Discriminative Models
- Generative models model $p(x,t)$. For example: Naive Bayes.
- Discriminative models model $p(t|x)$. For example: Logistic Regression.

Generative models involve more computations but with the flexibility to, for example, learn $p(x)$, which can be used for outlier detection. Discriminative models often have less application but are computationally more efficient than generative models.

=== Over-fitting
*Over-fitting* means the model is tuned to the random noise on the target values $t_i$, and hence would have sub-optimal performance on test data sets. There are two ways to address this issue:
- In the Frequentist framework, regularization is often used, e.g., regression with L-1 regularization (a.k.a. ``Lasso Regression''), and with L-2 regularization (a.k.a. ``Ridge Regression''). The regularization parameter is often tuned with a *validation data set*, or using the *cross-validation* technique if data are not abundant.
- In the Bayesian framework, the effective number of parameters adapts automatically (/at the cost of more computations/) to the size of the data set. In some cases, it is equivalent to the Frequentist method.

=== Curse of Dimensionality
The curse of dimensionality implies that we need significantly large quantity of data points to fit our model. This is because that, for a given number of data points, the higher the dimension, the lower the probability that they would reside close together, which breaks the homogeneity assumption of statistical models.

=== Information Theory
- The maximum entropy distribution in a closed interval is uniform.
- The maximum entropy distribution given its mean and variance is Gaussian.
- KL divergence is asymmetric.
- Entropy $H(p)$ is concave in $p$.
- Mutual information $I(x,y)$ is concave of $p(x)$ given $p(y|x)$ and convex of $p(y|x)$ given $p(x)$.


