\part{Theory}
\chapter{Preliminaries}
\deff{
	A {\bf{field}} is a non-empty set $F$ {\em{closed}} under two operations, usually called {\em{addition}} and {\em{multiplication}}\footnote{Subtraction and division are defined implicitly in terms of the inverse operations of addition and multiplication.}, and denoted by $+$ and $\cdot$ respectively, such that the following {\em{nine}} axioms hold
	\begin{enumerate}
		\item[(1-2).] Associativity of addition and multiplication.
		\item[(3-4).] Commutativity of addition and multiplication.
		\item[(5-6).] Existence and uniqueness of additive and multiplicative identity elements.
		\item[(7-8).] Existence and uniqueness of additive inverses and multiplicative inverses.
		\item[(9).] Distributivity of multiplication over addition.
	\end{enumerate}
}
\deff{
	The characteristic of a ring $R$, $char(R)$, is the smallest positive integer $n$ such that
	$$\underbrace{1+\cdots+1}_{n \text{ summands}} = 0$$
}
\thm{
	Any finite ring has nonzero characteristic.
}

\chapter{Vector Calculus}
\section{Vector Algebra}
\subsection{Dot Product}
\subsection{Cross Product}
\subsection{Scalar Triple Product}
\subsection{Vector Triple Product}

\section{Line, Surface, and Volume Integrals}

\chapter{Vector Spaces}
\section{Vector Space}
\deff{
	A {\bf{vector space}} over a field $\FF$ is a {\em{nonempty}} set $V$ together with the operations of addition $V\times V \to V$ and scalar multiplication $\FF \times V \to V$ satisfying the following {\em{eight}} properties:
	\begin{enumerate}[(-)]
		\item Additive axioms. For every $\u,\v,\w \in V$, we have
		\begin{enumerate}[(1)]
			\item $\u+\v = \v+\u$
			\item $(\u+\v)+\w = \u+(\v+\w)$
			\item ${\bf{0}}+\u = \u+{\bf{0}}=\u$, where ${\bf{0}} \in V$ is unique for all $\u \in V$
			\item $(-\u)+\u = \u+(-\u) = {\bf{0}}$, where $-\u \in V$ is unique for every $\u \in V$
		\end{enumerate}
		\item Multiplicative axioms. For every $\u \in V$ and scalars $a, b \in \FF$, we have
		\begin{enumerate}[(1)]
			\item $1\x = \x$
			\item $(ab)\x = a(b\x)$
		\end{enumerate}
		\item Distributive axioms. For every $\u, \v \in V$ and scalars $a, b \in \FF$, we have
		\begin{enumerate}[(1)]
			\item a(\u+\v) = a\u + a\v
			\item (a+b)\u = a\u + b\u
		\end{enumerate}
	\end{enumerate}
}
\section{Subspaces}
\deff{
	A subspace of $\RR^n$ is any collection $S$ of vectors in $\RR^n$ such that
	\begin{enumerate}[(1)]
		\item The zero vector $\mathbf{0}$ is in $S$.
		\item If $\u$ and $\v$ are in $S$, then $\u+\v$ is in $S$. \footnote{$S$ is closed under addition.}
		\item If $\u$ is in $S$ and $c$ is a scalar, then $c\u$ is in $S$. \footnote{$S$ is closed under scalar multiplication.}
	\end{enumerate}
}

\deff{
	Let $S, T$ be two subspaces of $\RR^n$. We say $S$ is orthogonal to $T$ if {\em{every}} vector in $S$ is orthogonal to {\em{every}} vector in $T$. The subspace $\{\mathbf{0}\}$ is orthogonal to all subspaces. \footnote{A line can be orthogonal to another line, or it can be orthogonal to a plane, but a plane cannot be orthogonal to a plane.}
}

\deff{
	Let $A$ be an $m \times n$ matrix.
	\begin{enumerate}[(1)]
		\item The {\em{row space}} of $A$ is the subspace $row(A)$ of $\RR^n$ spanned by the rows of $A$.
		\item The {\em{column space}} (or {\em{range}}) of $A$ is the subspace $col(A)$ of $\RR^m$ spanned by the columns of $A$.
	\end{enumerate}
}
\subsection{Four Important Subspaces: the row, column, null, and left null space}
\deff{
	Let $A$ be an $m \times n$ matrix. The {\em{null space}} (or {\em{kernel}}) of $A$ is the subspace of $\RR^n$ consisting of solutions of the homogeneous linear system $A\x=\mathbf{0}$. It is denoted by {\em{null($A$)}}.
}
\deff{
	A {\em{basis}} for a subspace $S$ of $\RR^n$ is a set of vectors in $S$ that
	\begin{enumerate}[(1)]
		\item spans $S$ and 
		\item is linearly independent. \footnote{It does not mean that they are orthogonal.}
	\end{enumerate}
}
\deff{
	If $S$ is a subspace of $\RR^n$, then the number of vectors in a basis for $S$ is called the {\em{dimension}} of $S$, denoted {\em{dim $S$}}. \footnote{The zero vector $\mathbf{0}$ is always a subspace of $\RR^n$. Yet any set containing the zero vector is linearly dependent, so $\mathbf{0}$ cannot have a basis. We define {\em{dim $\mathbf{0}$}} to be 0.}
}
\deff{
	The {\em{rank}} of a matrix $A$ is the dimension of its row and column spaces and is denoted by {\em{rank($A$)}}. \footnote{The row and column spaces of a matrix $A$ have the same dimension.}
}
\deff{
	The {\em{nullity}} of a matrix $A$ is the dimension of its null space and is denoted by {\em{nullity($A$)}}.
}
\thm{
	The Rank Theorem. If $A$ is an $m \times n$ matrix, then $$rank(A) + nullity(A) = n$$.
}
\thm{
	If $A$ is invertible, then $A$ is a product of elementary matrices.
}
\thm{
	Let $A$ be an $m \times n$ matrix. Then $rank(A^TA) = rank(A)$.
}
\deff{
	Let $S$ be a subspace of $\RR^n$ and let $B=\{\v_1,\cdots,\v_k\}$ be a basis for $S$. Let $\v$ be a vector in $S$, and write $\v = c_1\v_1 + \cdots + c_k\v_k$. Then $c_1,\cdots,c_k$ are called the coordinates of $\v$ with respect to $B$, and the column vector $$[\v]_B = [c_1,\cdots,c_k]^T$$ is called the coordinate vector of $\v$ with respect to $B$. \footnote{This coordinate vector is unique.}
}
\deff{
	A transformation $T: \RR^n \to \RR^m$ is called a linear transformation if $$T(c_1\v_1 + c_2\v_2) = c_1T(\v_1) + c_2T(\v_2)$$ for all $\v_1, \v_2$ in $\RR^n$ and scalars $c_1, c_2$.
}
\section{Bases and Dimension}
\section{Coordinates}

\chapter{Vector and Matrix Calculus}
\section{Functions of Vectors}
\subsection{Inner Product}
\subsection{Outer Product}
\deff{$$\u \otimes \v = \u \v^T$$}
\rmk{The inner product is the trace of the outer product.}
\subsection{Cross Product}
\deff{$$\a \times \b = \|\a\| \|\b\| sin(\theta) \n$$ It is also called the vector product.}
\section{Functions of Matrices}
\subsection{Matrix Determinant}


\subsection{Matrix Exponential}



\section{Functions of Vectors and Matrices}
\subsection{Linear Forms: One Vector as Argument}
\subsection{Bilinear and Quadratic Forms: Two Vectors as Argument}
\section{Derivatives of Vectors and Matrices}
\subsection{Derivatives of a Vector or Matrix with Respect to a Scalar}
Let $\A$ be a matrix, as a matrix-valued function
\begin{align}
	\A(x): \RR \rightarrow \RR^{m \times n}
\end{align}

For vector- and matrix-valued functions there is a further manifestation of the linearity of the derivative: Suppose that $f$ is a fixed linear function defined on $\RR^n$ and that $\A$ is a differentiable vector- or matrix-valued function. Then
\begin{align}
	f(\A)' = f(\A')
\end{align}
A useful example is the trace of $\A$, which is the sum of the diagonal elements of $\A$ (differentiable real-valued functions)
\begin{align}
	tr(\A)' = tr (\A')
\end{align}

Another example is the inner product of two vectors, where we have \footnote{Actually, it should work for all dot product (not necessarily the inner product, which is in the context of Euclidean spaces.)}
\begin{align}
	(\a^T\b)' = \a'^T\b + \a^T\b'
\end{align}

An important derivative of a matrix $\A$ is the derivative of its inverse.
\thm{
	$$\left(\A^{-1}\right)' = -\A^{-1} \A' \A^{-1}$$
}
\prof{
	Since $$\frac{\A^{-1}(x+h) - \A^{-1}(x)}{h} = \frac{\A^{-1}(x+h)[\A(x+h)-\A(x)]\A^{-1}(x)}{h}$$

	Another easy proof is: $$\mathbf{0} = \I' = (\A^{-1}\A)' = (\A^{-1})'\A + \A^{-1}\A'$$ Post-multiply $\A^{-1}$ and obtain the desired proof.
}
\section{Integration of Vectors and Matrices}

\chapter{Some Intuitive Explanations}
\section{Eigenvalues and Singular Values}
\section{SVD, PCA, and Change of Basis}