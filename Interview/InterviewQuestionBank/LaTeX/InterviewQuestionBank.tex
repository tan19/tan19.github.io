\documentclass[oldfontcommands]{memoir}
\input{LaTeX/LaTeXMacros.tex}
\usepackage{verbatim}

\counterwithin*{part}{book}
\counterwithin*{chapter}{book}

\title{Quant Interview Guide}
\author{emc57721@gmail.com}

\makeindex[options= -s LaTeX/index_style.ist]

\dominitoc

\begin{document}
{
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
%\shorttoc{List of Chapters}{0} % Only chapters

\adjustmtc\part{Brainteaser}
\chapter{Logic Brainteaser}
\minitoc
\section{Games}
\begin{question}{Poker Game}{Easy}{\#1002.md; Brainteaser, Logic Brainteaser; Baidu}
Two people play a deck of 52 cards. Each person in turn draws 1 to 4 cards and compare the number. What is your strategy if you are to draw first?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Poker Game}
\end{answer}
\chapter{Math Brainteaser}
\minitoc
\section{Games}
\begin{question}{Two Strings}{Easy}{\#1101.md; Brainteaser, Math Brainteaser; Microsoft}
You have two strings. Each string takes exactly one hour to burn. The rate at which the strings are burnt is completely random and the two strings are different. How do you measure 45 minutes with these two strings?

\end{question}

\begin{question}{Coin Game}{Easy}{\#1102.md; Brainteaser, Math Brainteaser; Baidu}
Two people take 1-5 coins in turn from a pile of 100 coins. Is there a strategy to guarantee that the first person to pick always get the last coin?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Two Strings}
45 = 30 + 15. Burn one string (pick any one of the two) at both ends, and only one end of the other string. As soon as the first string is finished (which takes 30 minutes), burn the other end of the second string. It will take additional 15 minutes for the second string to finish. The total time is 45 minutes.\end{answer}
\begin{answer}{Coin Game}
\end{answer}
\part{Mathematics}
\chapter{Calculus}
\minitoc
\section{Comparison}
\begin{question}{Compare cos and sin}{Easy}{\#2005.md; Calculus; NA}
Compare $\cos(\sin x)$ and $\sin(\cos x)$.

\end{question}

\begin{question}{Compare e and pi}{Easy}{\#2008.md; Calculus; 150 Book}
Which number is larger, $\pi^e$ or $e^\pi$?

\end{question}

\section{Derivative}
\begin{question}{Calculate Integral}{Easy}{\#2006.md; Calculus; Green Book}
What is the integral of $\sec(x)$ from 0 to $\pi/6$?

\end{question}

\begin{question}{Evaluate Polynomial}{Easy}{\#2007.md; Calculus; World Quant}
Consider a polynomial $f(x)$ and its derivative $f'(x)$ that are related according to $$f(x) - f'(x) = x^3 + 3x^2 + 3x + 1.$$  What is $f(9)$?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Compare cos and sin}
\begin{align*}
    f(x) &\equiv \cos\sin x - \sin \cos x\\
    &= \cos \sin x - \cos \left(\frac{\pi}{2} - \cos x\right)	\\
    &= -2\sin\left(\frac{\sin x + (\frac{\pi}{2} - \cos x)}{2}\right)\sin\left(\frac{\sin x - (\frac{\pi}{2} - \cos x)}{2}\right)\\
    &= -2\sin\left(\frac{\frac{\pi}{2} + \sqrt{2}\sin(x - \pi/4)}{2}\right)\sin\left(\frac{-\frac{\pi}{2} + \sqrt{2}\sin(x + \pi/4)}{2}\right)	
\end{align*}		
Since $-1 \le \sin(x \pm \pi/4) \le 1$ and $2\sqrt{2} < \pi$, we have,
\begin{align*}
    0 &< \frac{\frac{\pi}{2} + \sqrt{2}\sin(x - \pi/4)}{2} < \frac{\pi}{2}	\\
    -\frac{\pi}{2} &< \frac{-\frac{\pi}{2} + \sqrt{2}\sin(x + \pi/4)}{2} < 0
\end{align*}
And finally,
\begin{align*}
    f(x) \equiv \cos\sin x - \sin \cos x > 0
\end{align*}
Note that, the equality is not attained.\end{answer}
\begin{answer}{Calculate Integral}
NA\end{answer}
\begin{answer}{Evaluate Polynomial}
Since $f(x)$ is a polynomial, so we may write
\begin{align*}
    f(x) &= ax^3 + bx^2 + cx + d\\
    f'(x) &= 3ax^2 + 2bx + c\\		
    f(x) - f'(x) &= ax^3 + (b-3a)x^2 + (c-2b)x + (d-c)	
\end{align*}
which implies $a = 1, b= 6, c = 15, d = 16$. Hence,
\begin{align*}
    f(x) = x^3 + 6x^2 + 15x + 16.
\end{align*}
\end{answer}
\begin{answer}{Compare e and pi}
Take log on both sides we have $$\frac{\log \pi}{\pi} < \frac{\log e}{e},$$ because $$\left(\frac{\log x}{x}\right)' = - \frac{\log x + 1}{x^2} < 0$$ for $x > e$. Therefore, $e^\pi > \pi^e$.\end{answer}
\chapter{Combinatorics and Probability}
\minitoc
\section{Expectation}
\begin{question}{All Six Sides of A Die}{Easy}{\#2101.md; Expectation; NA}
What is the expected number of tosses you need to see all six sides of a die?

\end{question}

\begin{question}{First Ace}{Easy}{\#2102.md; Expectation; Green Book, page 95}
You turn over a card one by one from a deck of 52 cards. What is the expected number of cards that you need to flip before you see the first ace?

\end{question}

\begin{question}{Hand Shakes}{Easy}{\#2112.md; Indicator Variables; Linearity of Expectations; NA}
There are $x$ people from country A and y people from country B. They sit around a table and shake hands with people on their left and right, but they only shake hands if they're from the same country. We want to know the expected number of handshake made.

\end{question}

\begin{question}{Hunters and Ducks}{Easy}{\#2113.md; Indicator Variables; Linearity of Expectations; Morgan Stanley}
There are 10 hunters and 10 ducks flying over. Each hunter aims at one duck and there is a probability of 0.5 that a hunter kills the duck he aims at. What is the expected number of ducks that have survived?

\end{question}

\section{Conditional Probability}
\begin{question}{Bayesian Child}{Easy}{\#2103.md; Bayes Theorem; Conditional Probability; SIG}
In a hospital there were 3 boys and some girls. A woman gave birth to a child in the hospital. A nurse picked up a child at random and was a boy. What is the probability that that woman gave birth to a boy?

\end{question}

\section{Coin Games}
\begin{question}{Coin Game}{Easy}{\#2111.md; Coin Games; SIG}
You and I each flip 3 fair coins, if we got same heads I pay you \$2, if different you pay me \$1. Will you play this game?

\end{question}

\section{Deterministic Games}
\begin{question}{Robot Flipping Coins}{Easy}{\#2114.md; Deterministic Games; Markov Chain; Optiver}
In a room there are half H and half T. A robot comes into the room and flips the Hs and tosses the Ts, repeatedly. What is the stationary distribution of the Hs and Ts in the room?

\end{question}

\begin{question}{Coupon Collection}{Easy}{\#2115.md; Dice Games; Deterministic Games; Green Book; Black Book; Jump;}
Expected number of die rolls until all numbers are rolled? If I have already made $N$ rolls, what is the expected number of distinctive numbers?

\end{question}

\begin{question}{Two Heads}{Easy}{\#2116.md; Think Backwards; Symmetry; Coin Games; NA}
The expected number of flips to see two heads from a series of fair coin tosses.

\end{question}

\begin{question}{One More Coin}{Easy}{\#2117.md; Think Backwards; Symmetry; Coin Games; Green Book; Blue Book; HRT;}
A tosses a fair coin $n+1$ times; B tosses a fair coin $n$ times. What is the probability that A has strictly more heads than B?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{All Six Sides of A Die}
The answer is $6 \cdot \sum_{i=1}^6 1/i$. Let $X_i$ denote the random variable that tells us the number of trials it takes for us to see the i-th distinct side on a die. Obviously, $X_1$ is always 1, since the first roll gives us the first observed side. $X_2$ follows a geometric distribution with success p = 5/6. Similarly $X_i$ follows a geometric distribution with success p = (7-i)/6. So, the total expected number of rolls is E[X1 + ... + X6] = E[X1] + ... E[X6] = 6 * (1 + 1/2 + ... + 1/6).\end{answer}
\begin{answer}{First Ace}
The answer is 10.6. Setup an indicator function $X_i = \mbox{all aces are behind card } i$. Then total number of draws is $N = 1 + \sum_{i=1}^48 X_i$, where $p(X_i) = 1/5$. The 1/5 comes from the fact that each non-ace card has a probability of 1/5 being appear before all aces.\end{answer}
\begin{answer}{Bayesian Child}
\begin{align}
	P(\text{Boy} ~|~ \text{Picked a Boy}) &= \frac{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy})}{P(\text{Picked a Boy})}\\
	 &= \frac{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy})}{P(\text{Picked a Boy} ~|~ \text{Boy})P(\text{Boy}) + P(\text{Picked a Boy} ~|~ \text{Girl})P(\text{Girl})}\\
	 &= \frac{\frac{4}{3+N+1} \cdot \frac{1}{2}}{\frac{4}{3+N+1} \cdot \frac{1}{2} + \frac{3}{3+N+1} \cdot \frac{1}{2}}\\
	 &= \frac{4}{4+3} = \frac{4}{7}
\end{align}\end{answer}
\begin{answer}{Coin Game}
Naive Way:
\begin{align}
	P(A=B=0) &= \left[\binom{3}{0} \left(\frac{1}{2}\right)^3\right]^2 = \frac{1}{64}\\
	P(A=B=1) &= \left[\binom{3}{1} \left(\frac{1}{2}\right)^3\right]^2 = \frac{9}{64}\\
	P(A=B=2) &= \left[\binom{3}{2} \left(\frac{1}{2}\right)^3\right]^2 = \frac{9}{64}\\
	P(A=B=3) &= \left[\binom{3}{3} \left(\frac{1}{2}\right)^3\right]^2 = \frac{1}{64}	
\end{align}
So
\begin{align}
	P(A=B) &= \frac{1}{64} + \frac{9}{64} + \frac{9}{64} + \frac{1}{64} = \frac{5}{16}
\end{align}
Don't forget the case when both had no head (A=B=0).

Clever Way: Let both do their tossing and then let B turn over each of her coins. Then the event you are looking for is that exactly three out of six coins show heads. Since B's "trick" doesn't destroy any randomness or independency, the answer is
\begin{align}
	P(A=B) &= \binom{6}{3} \left(\frac{1}{2}\right)^3 = \frac{5}{16}
\end{align}
\end{answer}
\begin{answer}{Hand Shakes}
For each seat, the probability the person sitting there shakes the hand to the left is
\begin{align}
	\frac{x}{x+y} \cdot \frac{x-1}{x+y-1} + \frac{y}{x+y} \cdot \frac{y-1}{x+y-1} = \frac{x(x-1) + y(y-1)}{(x+y)(x+y-1)}
\end{align}
Hence
\begin{align}
	E\left(\sum_i Z_i\right) = \sum_i Z_i = (x+y) \cdot \frac{x(x-1) + y(y-1)}{(x+y)(x+y-1)} = \frac{x(x-1) + y(y-1)}{x+y-1}
\end{align}\end{answer}
\begin{answer}{Hunters and Ducks}
The answer is 6 (but mine is 5?). Define a set of indicator variables:
\begin{align*}
    Z_i = \begin{cases} 
    0, ~~ \mbox{if the ith duck has survived}\\
    1, ~~ \mbox{if the ith duck has died}		
    \end{cases}
\end{align*}
where $i = 1, \dots, 10$. Additionally, we define $Pr(Y_j = i)$ to be the probability that the jth hunter aims at the ith duck. Now,
\begin{align*}
    \EEE\left[\sum_{i=1}^{10} Z_i\right] &= \sum_{i=1}^{10} \EEE[Z_i] = \sum_{i=1}^{10} Pr(Z_i=1)\\
    &= \sum_{i=1}^{10} \sum_{j=1}^{10} Pr(Z_i=1|Y_j = i) Pr(Y_j = i)\\
    &= \sum_{i=1}^{10} \sum_{j=1}^{10} (1/2)(1/10)\\
    &= 5		
\end{align*}\end{answer}
\begin{answer}{Robot Flipping Coins}
The answer is 1/3 for H and 2/3 for T. We model this as a Markov Chain, which has a transition matrix
\begin{align*}
    M = 
    \begin{pmatrix}
        0 & 1\\
        0.5 & 0.5
    \end{pmatrix}	
\end{align*}
Suppose in equilibrium, the probability of H is $p$ and the probability of T is $1-p$, then
\begin{align*}
    (p, 1-p) 
    \begin{pmatrix}
        0 & 1\\
        0.5 & 0.5
    \end{pmatrix}
    =
    (p, 1-p)
\end{align*}
This gives us $p = 1/3$. Alternatively, using eigendecomposition ($M = Q \Lambda Q^{-1}$), we find:
\begin{align*}
    M = 
    \begin{pmatrix}
    1 & 1\\
    1 & -1/2\\		
    \end{pmatrix}
    \begin{pmatrix}
    1 & 0\\
    0 & -0.5\\		
    \end{pmatrix}
    \begin{pmatrix}
    1/3 & 2/3\\
    2/3 & -2/3\\		
    \end{pmatrix}
\end{align*}
and 
\begin{align*}
    M^\infty = 
    \begin{pmatrix}
    1 & 1\\
    1 & -1/2\\		
    \end{pmatrix}
    \begin{pmatrix}
    1 & 0\\
    0 & 0\\		
    \end{pmatrix}
    \begin{pmatrix}
    1/3 & 2/3\\
    2/3 & -2/3\\		
    \end{pmatrix}
\end{align*}
This gives us the same result: $(0.5, 0.5) M^\infty = (1/3, 2/3)$.\end{answer}
\begin{answer}{Coupon Collection}
This is the famous ``Coupon Collection'' problem. Define a set of variables $X_1, \dots, X_6$ where $X_i$ denotes the number of additional rolls need to obtain the $i^{th}$ new number, then the answer reduces to computing $\EEE[\sum_{i=1}^6 X_i]$. However, each $X_i$ follows a Geometric distribution where $p_i = (6 - (i-1)) / 6$ and $\EEE[X_i] = 6 / (6 - (i-1))$. Therefore,
\begin{align*}
    \EEE\left[\sum_{i=1}^6 X_i\right] = \sum_{i=1}^6 \frac{6}{6- (i-1)} = 6(1/6 + 1/5 + 1/4 + 1/3 + 1/2 + 1) = 14.7
\end{align*}
In general, the answer for the coupon collection problem is:
\begin{align*}
\EEE = N\sum_{i=1}^N \frac{1}{i}
\end{align*}

For the second part, introduce the indicator variables $Y_i$ to denote that if number $i$ is in the rolls already made. Then
\begin{align*}
\EEE\left[\sum_{i=1}^6 Y_i \right] = \sum_{i=1}^6 \EEE[Y_i] = \sum_{i=1}^6 Pr(Y_i = 1) = \sum_{i=1}^6 1 - \left(\frac56\right)^N = 6\left[1 - \left(\frac56\right)^N\right]
\end{align*}\end{answer}
\begin{answer}{Two Heads}
Answer is 4 for to see two heads, and 6 to see two consecutive heads. Expected \# of flips to gets 1 H is just, E[X] = 1 + (1-p)(E[X]), where p is prob. of getting H, in the case of a fair coin this is 0.5, therefore rearranging you get E[X] = 1/p, meaning expected number of flips for 1 head is 1/p, for fair coin this is equal to 2. Since coin flipping is memoryless, expected number of flips to get k heads is just 1/p + 1/p + ... = k/p. Therefore 2/0.4 = 4, when k=2.\end{answer}
\begin{answer}{One More Coin}
The answer is 1/2. First observe that if A were to toss only $n$ times, then there are only three events:
\begin{enumerate}
    \item With probability $P_1$, A has more heads than B does.
    \item With Probability $P_2$, A has the same number of heads compared to B.
    \item With probability $P_3$, A has fewer heads than B does.
\end{enumerate}
By symmetry, $P_1 = P_3$ and $P_1 + P_2 + P_3 = 1 \implies 2P_1 + P_2 = 1$. The additional coin toss A has increases the probability of A having more heads by just $0.5P_3$, hence the probability in question is $P_1 + 0.5P_2 = 0.5$.\end{answer}
\chapter{Linear Algebra}
\minitoc
\section{Positive Definiteness and Covariance Matrices}
\begin{question}{How to check positive definiteness of a matrix?}{Easy}{\#2002.md; Math, Matrix, Linear Algebra, Positive Definite; NA}
How to check if a matrix is positive definite? How about positive semi-definite?

\end{question}

\section{Trace}
\begin{question}{Trace of Matrix Sum}{Easy}{\#2004.md; Trace; Eigenvector; Eigenvalue;; NA}
Given a $3 \times 3$ matrix $\M$, what is $\mbox{tr}\left(\sum_{k=0}^\infty \M^k\right)$?

\end{question}

\section{Determinant}
\begin{question}{Sherman-Morrison Formula}{Easy}{\#2009.md; Calculus; Sherman-Morrison Formula; World Quant}
Prove the Sherman-Morrison formula: $$(\I + \u\v^T)^{-1} = \I - \frac{\u\v^T}{1 + \u\v^T}$$ and the Matrix Determinant Lemma: $$\det(\I + \u\v^T) = 1+\v^T\u.$$	

\end{question}

\section{Matrix Decomposition}
\begin{question}{QR Decomposition}{Easy}{\#2010.md; Trace; QR Decomposition; NA}
How do you use QR decomposition in linear regression?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{How to check positive definiteness of a matrix?}
Need to check if the matrix is square first.\end{answer}
\begin{answer}{Trace of Matrix Sum}
	The answer is
	\begin{align*}
	\mbox{tr}\left(\sum_{k=0}^\infty \M^k\right) = \sum_{i=1}^3 \frac{1}{1 - \lambda_i}
	\end{align*}
	where $\lambda_i$ are the eigenvalues of $\M$. The key is that trace is the sum of all eigenvalues, and if $\lambda$ is an eigenvalue of $\M$ then $\lambda^k$ is an eigenvalue of $\M^k$. This is easily seen by noticing that
	\begin{align*}
		\M\x = \lambda\x \implies \M^2\x = \lambda(\M\x) = \lambda^2 \x
	\end{align*}
	Now, 
	\begin{align*}
	\mbox{tr}\left(\sum_{k=0}^\infty \M^k\right) = \sum_{k=0}^\infty \mbox{tr}(\M^k) = \sum_{k=0}^\infty \sum_{i=1}^n \lambda_i^k = \sum_{i=1}^n \sum_{k=0}^\infty \lambda_i^k = \sum_{i=1}^n \frac{1}{1 - \lambda_i}
	\end{align*}
	The answer is given by $n = 3$.
\end{answer}
\begin{answer}{Sherman-Morrison Formula}
We guess the inverse has the similar form $(\I + \u\v^T)^{-1} = \I + \alpha\v^T\u$, thus
\begin{align*}
(\I + \u\v^T)(\I + \alpha\u\v^T) &= \I + a\u\v^T + \u\v^T + a\u\v^T\u\v^T = \I + \u(a+1+a\v^T\u)\v^T \\
&\implies a = -\frac{\u\v^T}{1+\u^T\v}
\end{align*}
and hence $$(\I + \u\v^T)^{-1} = \I - \frac{\u\v^T}{1 + \u^T\v}.$$	

If we look at the eigenvalues,
\begin{align*}
    (\I + \u\v^T)\u = \u + \u\v^T\u = (1 + \v^T\u)\u
\end{align*}
We see that $\u$ is an eigenvector and $1 + \v^T\u$ is its eigenvalue. Moreover, there are $n-1$ orthogonal vectors (suppose $\I$ is of dimension $n$) $\b$ such that $\v^T\b = 0$, and for each of them we have $(\I + \u\v^T)\b = \b$, which suggests that $1$ is the other eigenvalue with multiplicity $n-1$. Therefore, the determinant of $\I + \u\v^T$ is the product of its eigenvalues, which is $1 + \u^T\v$. That is
\begin{align*}
    \det(\I + \u\v^T) = 1 + \u^T\v
\end{align*}
This proves the Matrix Determinant Lemma.\end{answer}
\begin{answer}{QR Decomposition}
Recall that the normal equation 
\begin{align*}
    \bbeta = (\X^T\X)^{-1} \X^T \y
\end{align*}
involves the inverse of $\X^T\X$, which is numerically unstable when the matrix is near singular and ill-conditioned, i.e., the condition number $$\kappa(\A) = \frac{\sigma_{\max}(\A)}{\sigma_{\min}(\A)} = ||\A^{-1}| \cdot |||\A||$$ is large. Now, suppose we have a \emph{thin} QR factorization of the design matrix (usually $n \ge p$) $$\X_{n \times p} = \Q\R = [\Q_1, \Q_2]\begin{pmatrix}\R_1\\ \bZero\end{pmatrix} = \Q_1\R_1,$$ where $Q_1, Q_2$ both have orthonormal columns but with sizes $n \times p$ and $n \times (n-p)$, respectively. $\R_1$ is an upper triangular matrix with size $p \times p$.  Then since orthogonal matrices preserve the 2-norm, we have:
\begin{align*}
    &\min_\bbeta \frac12||\X\bbeta - \y||_2^2 \\
    \implies & \min_\bbeta \frac12||\Q_1\R_1\bbeta - \y||_2^2\\
    \implies & \min_\bbeta \frac12||\Q_1^T(\Q_1\R_1\bbeta - \y)||_2^2\\
    \implies & \min_\bbeta \frac12||\R_1\bbeta - \Q_1^T\y||_2^2\\		
\end{align*}
Since $\R_1$ is upper triangular, we only need one backward sweep and one forward sweep to solve $\bbeta$.
\end{answer}
\part{Finance}
\chapter{Quant Finance}
\minitoc
\section{Stochastic Calculus}
\begin{question}{NA}{Easy}{\#3001.md; Stochastic Calculus; NA}
NA

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{NA}
NA
\end{answer}
\part{Computer Science}
\chapter{C++}
\minitoc
\section{Debug}
\begin{question}{Code Debug}{Easy}{\#4001.md; C++, Debug; NA}
What's wrong with the following code?

\begin{verbatim}
class A {
private:
    int value;
public:
    A(int n) { value = n; }
    A(A other) { value = other.value; }

    void Print() {std::cout << value << std::endl; }
};

int _tmain(int argc, _TCHAR* argv[]) {
    A a = 10;
    A b = a;
    b.Print();

    return 0;
}
\end{verbatim}


\end{question}

\section{Casting}
\begin{question}{C++ Cast}{Easy}{\#4002.md; Casting; NA}
Describe all the C++ casts and their properties.

\end{question}

\section{Shallow Copy and Deep Copy}
\begin{question}{Shallow Copy vs Deep Copy}{Easy}{\#4003.md; Deep Copy, Shallow Copy; NA}
What is {\bf{shallow copy}} and {\bf{deep copy}}? What member function needs to be implemented in order to implement {\bf{deep copy}}?

\end{question}

\section{Class}
\begin{question}{Empty Class}{Easy}{\#4004.md; Computer Science, Empty Class; Jian Zhi Offer}
What is the {\bf{sizeof}} of an empty class? Follow-up: 1) why not zero? 2) what if I add a constructor and a destructor? 3) what if the destructor is virtual? 4) what if I add an int and a double? 5) what if I add an int and a double?

\end{question}

\begin{question}{Empty Class}{Easy}{\#4008.md; Class; NA}
What will be implemented for an empty class?

\end{question}

\section{Private Member Variable}
\begin{question}{Private Member Variable}{Easy}{\#4005.md; Private Member Variable; NA}
You can only read the source code of a class, which has a private member variable. How can you get the access of the private member variable?

\end{question}

\section{Value Type and Reference Type}
\begin{question}{Value Type and Reference Type}{Easy}{\#4006.md; Value Type and Reference Type; Jian Zhi Offer}
Explain {\bf{value type}} and {\bf{reference type}}.

\end{question}

\section{STL}
\begin{question}{Vector vs List}{Easy}{\#4007.md; STL; NA}
What is the difference between {\bf{vector}} and {\bf{list}}? How does {\bf{vector}} expand in size, why not use 2x?

\end{question}

\begin{question}{Map vs Unorder Map}{Easy}{\#4009.md; STL; NA}
What is the difference between {\bf{map}} and {\bf{unorder map}}?

\end{question}

\begin{question}{Six Key Components of STL}{Easy}{\#4015.md; C++, STL; NA}
What are the six key components of STL? Explain.

\end{question}

\begin{question}{STL Map}{Easy}{\#4016.md; C++, STL; NA}
Are data in STL map ordered?

\end{question}

\begin{question}{Map vs Set}{Easy}{\#4017.md; C++, STL; NA}
What is the difference between {\bf{map}} and {\bf{set}}?

\end{question}

\section{Multithreading}
\begin{question}{Select, Poll and Epoll}{Easy}{\#4010.md; Multithreading; NA}
What is the difference between {\bf{select}}, {\bf{poll}} and {\bf{epoll}}?

\end{question}

\section{Memory}
\begin{question}{New and Malloc}{Easy}{\#4011.md; C++, Memory; NA}
What is the difference between {\bf{new}} and {\bf{malloc}}?

\end{question}

\section{Virtual Function}
\begin{question}{Modify Virtual Function of Parent Class}{Easy}{\#4012.md; C++, Virtual Function; Tencent}
Can a derived class modify the default parameters of a virtual function of a parent class? If so, at compile time or run time?

\end{question}

\section{Pointers and References}
\begin{question}{Pointers and References}{Easy}{\#4013.md; C++, Pointers and References; NA}
What is the difference between pointers and references? Does reference take memory space?

\end{question}

\section{Smart Pointers}
\begin{question}{Smart Pointers}{Easy}{\#4014.md; C++, Smart Pointers; NA}
Explain smart pointers.

\end{question}

\section{Virtual Functions}
\begin{question}{When to use a virtual destructor?}{Easy}{\#4018.md; Virtual Destructor; Ubiquant}
When to use a virtual destructor?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Code Debug}
Should be
\begin{verbatim}
A(const A& other)
\end{verbatim}
\end{answer}
\begin{answer}{C++ Cast}
\end{answer}
\begin{answer}{Shallow Copy vs Deep Copy}
\end{answer}
\begin{answer}{Empty Class}
(Usually) 1 Byte.

An object has to occupy some physical memory space, otherwise when you instantiate it you wouldn't be able to use it. The size of an empty class is usually 1 byte, but it depends on the compiler and system.

If we add a constructor and a destructor, the {\bf{sizeof}} will still be one byte. If the destructor is virtual, the {\bf{sizeof}} will be 8 bytes (in a x386-64 machine, which is 64-bit), since a {\bf{vpointer}} will be added to point to a {\bf{vtable}}.

If we add an int, the size will be 4 (in a x386-64 machine), but if we add one more double, the size will be 16, although a double takes 8 bytes and an int take 4 bytes. This is because of {\bf{padding}}, i.e., shorter types will be padded to align with the longest type.\end{answer}
\begin{answer}{Private Member Variable}
\end{answer}
\begin{answer}{Value Type and Reference Type}
\end{answer}
\begin{answer}{Vector vs List}
\end{answer}
\begin{answer}{Empty Class}
\end{answer}
\begin{answer}{Map vs Unorder Map}
\end{answer}
\begin{answer}{Select, Poll and Epoll}
\end{answer}
\begin{answer}{New and Malloc}
\end{answer}
\begin{answer}{Modify Virtual Function of Parent Class}

\end{answer}
\begin{answer}{Pointers and References}

\end{answer}
\begin{answer}{Smart Pointers}

\end{answer}
\begin{answer}{Six Key Components of STL}
- containers
- algorithm
- iterator
- adapter
- allocator
- functor \end{answer}
\begin{answer}{STL Map}
\end{answer}
\begin{answer}{Map vs Set}
\end{answer}
\begin{answer}{When to use a virtual destructor?}
Need to check if the matrix is square first.

\end{answer}
\chapter{Compiler}
\minitoc
\section{Compiling Process}
\begin{question}{Compiling Process}{Easy}{\#4101.md; Compiler; NA}
What are the stages of compiling?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Compiling Process}
- Macro Substitution
- Syntactic Analysis
- Grammatical Analysis
- Quadruplet Generation
- Table of Variable Maintenance\end{answer}
\chapter{Database}
\minitoc
\section{Database}
\begin{question}{B Tree vs B+ Tree}{Easy}{\#4301.md; Database; NA}
What is the difference between {\bf{B Tree}} and {\bf{B+ Tree}}? WHy do we need B+ tree?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{B Tree vs B+ Tree}
\end{answer}
\chapter{Operating System}
\minitoc
\section{Thread}
\begin{question}{Process vs Thread}{Easy}{\#4201.md; Operating System; NA}
What is the difference between {\bf{process}} and {\bf{thread}}?

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Process vs Thread}
\end{answer}
\part{Algorithm and Optimization}
\chapter{Optimization}
\minitoc
\section{Root Finding}
\begin{question}{Newton's Method}{Easy}{\#5001.md; Newton's Method; NA}
Explain Newton's method.

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Newton's Method}
Suppose we have a function $f(x) = 0$ and we want to find its root. We start with $x_0$ close to its root, and update as
\begin{align}
    x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}
\end{align}
\end{answer}
\chapter{Algorithm}
\minitoc
\section{Dynamic Programming}
\begin{question}{NA}{Easy}{\#5101.md; Data Science; NA}
NA

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{NA}
NA
\end{answer}
\part{Data Science}
\chapter{Data Science}
\minitoc
\section{Data Science}
\begin{question}{Naive Bayes vs Logistic Regression}{Easy}{\#6001.md; Naive Bayes; Logistic Regression; NA}
Compare {\bf{Naive Bayes}} and {\bf{Logistic Regression}}.

\end{question}

\newpage\section{Chapter Answers}\begin{answer}{Naive Bayes vs Logistic Regression}
NB has high bias as it focuses on a smaller set of models, converges faster but error rate may be high if assumptions are incorrect. Logistic Regression has lower bias as it allows a larger model space, converges to solutions slower but error rate can be lower due to low bias. Logistic Regression tends to be used a LOT more in industry but if your prior information is accurate (data is generated from a well known process), NB may be a better choice.

NB assumes that the input features are conditional  independent. NB also requires a prior distribution. Simple LR cannot do nonlinear classification.\end{answer}
\addcontentsline{toc}{chapter}{Index}
\printindex

\end{document} 