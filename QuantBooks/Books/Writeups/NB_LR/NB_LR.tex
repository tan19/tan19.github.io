\documentclass{article}
\input{/Users/tan19/Dropbox/LaTeXMacros.tex}

\title{Naive Bayes and Logistic Regression}
\author{Xi Tan (tan19@purdue.edu)}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\section{Logistic Regression}
The logistic function is an important special case of sigmoid functions, a family of functions that are ``S''-shaped. It is defined as:
\begin{align}
	\sigma(t) := \frac{1}{1+e^{-t}},\quad t \in \RRR
\end{align}
For binary classification, we can rewrite
\begin{align}
	P_{\W}(\CC = 1 | \x) &:= \sigma((\w_1-\w_2)^T\x)\\
	&= \frac{1}{1+\exp(-(\w_1-\w_2)^T\x)}\\
	&= \frac{\exp(\w_1^T\x)}{\exp(\w_1^T\x)+\exp(\w_2^T\x)}
\end{align}
where the class weight matrix $\W$ has two columns $\w_1$ and $\w_2$, the weight vector for class $1$ and $2$, respectively. The softmax function generalizes the logistic function to multiple classes:
\begin{align}
	P_{\W}(\CC = i | \x) &:= \frac{\exp(\w_i^T\x)}{\sum_{k=1}^K\exp(\w_k^T\x)}
\end{align}
where $\W$ has $K$ columns, each of which is the corresponding weight vector.




\end{document}
