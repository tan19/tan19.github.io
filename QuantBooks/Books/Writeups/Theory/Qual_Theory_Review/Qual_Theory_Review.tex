\documentclass{report}
\usepackage{amssymb,amsmath}
\usepackage[thmmarks,amsmath]{ntheorem}

\usepackage{hyperref}

\theorembodyfont{\normalfont}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\title{Qualifying Exam Preparation III \\Theory of Statistics}
\author{Xi Tan (tan19@purdue.edu)}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

\chapter*{Preface}
STAT 528:  References: Bickel \& Doksum (Mathematical Statistics: Basic Ideas and Selected Topics, 2nd ed.); Casella \& Berger (Statistical Inference, 2nd ed. ).

STAT 553:   References; Seber and Lee (Linear Regression Analysis, 2nd ed.) Stapleton (Linear Statistical Models), Christensen (Plane Answers to Complex Questions),  Schervish (Theory of Statistics). 

TBD

\part{STAT528 Topics}
\chapter{Data, Models, Statistics, Parameters}
\begin{definition}
	The set, $S$, of all possible outcomes of a particular experiment is called the sample space for the experiment.
\end{definition}
\begin{definition}
	An event is any collection of possible outcomes of an experiment, that is, any subset of $S$ (including $S$ itself).
\end{definition}
\begin{definition}
	A random variable is a function from a sample sapce $S$ into the real numbers.
\end{definition}


\section{Distributions of Functions of a Random Variable}
\begin{theorem}
	From Casella \& Berger Theorem 2.1.5) Let $X$ have pdf $f_X(x)$ and let $Y=g(X)$, where $g$ is a monotone function. Suppose that $f_X(x)$ is continuous and that $g^{-1}(y)$ has a continuous derivative. Then the pdf of $Y$ is given by
	\begin{align}
		f_Y(y) = f_X(g^{-1}(y)) \Big|\frac{d}{dy} g^{-1}(y)\Big|
	\end{align}
\end{theorem}

asdfasdf

\chapter{Decision Theory (Bayes and Minimax Criteria, Risk Functions, Estimation and Testing in Terms of the Decision Theoretic Framework}
\chapter{Bayesian Models, Conjugate (and Other) Prior Distributions}
\chapter{Prediction (Optimal MSPE and Optimal Linear MSPE)}
\chapter{Sufficiency (Factorization Theorem)}
\chapter{Natural Sufficient Statistics}
\chapter{Minimal Sufficiency}
\chapter{Estimation (Least Squares, MLE, Frequency Plug-in, Method of Moments, Combinations of These)}
\chapter{Exponential Families \& Properties, Canonical Exponential Families (\& Fisher Information)}
\chapter{Information Inequality, Fisher Information, UMVU Estimates, Cramer-Rao Lower Bound}
\chapter{Neyman-Pearson Testing Theory (Form, MP Test, UMP Test, MLR Family, Likelihood Ratio Tests)}
\chapter{Asymptotic Approximation / Large Sample Theory (Consistency, Delta Method, Asymptotic Normality of MLE, Slutsky's Theorem, Efficiency, Pearson's Chi-Square)}


\part{Stat 553 Topics}
\chapter{Linear Models, Estimable Functions, Least Squares Estimates=LSE, Normal Equations, Projections, Gauss Markov Theorem, BLUE}
\chapter{Multivariate Normal Distribution and Distribution of Linear and Quadratic Forms}
\chapter{Properties of LSE and Generalized LSE}
\chapter{General Linear Hypothesis=GLH, Testing of GLH}
\chapter{Orthogonalization of Design Matrix and Canonical Reduction of GLH;  Adding Variables To The Model}
\chapter{Correlation, Multiple Correlation and  Partial Correlation}
\chapter{Confidence Regions and Prediction Regions}
\chapter{Simultaneous Confidence Sets, Bonferroni, Scheffe Projection Method, Tukey Studentized Range}
\chapter{Introduction to Design of Experiments, ANOVA and ANOCOVA, Factorial and Block Designs, Random, Fixed and Mixed Models, Components of Variance}
\chapter{Hierarchial Bayes Analysis of Variance; (Schervish Ch. 8, 8.1,8.2) Partial Exchangeability and Hierarchical, Models, Examples and Representations, Normal One Way ANOVA and Two Way Mixed Model ANOVA}


\end{document} 






