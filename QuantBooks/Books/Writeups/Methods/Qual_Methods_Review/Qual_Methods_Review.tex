\documentclass{article}
\usepackage{amssymb,amsmath}
\usepackage[thmmarks,amsmath]{ntheorem}

\usepackage{mathrsfs}
\usepackage{hyperref}

\newcommand{\GP}{Gaussian process\xspace}
\newcommand{\GPs}{Gaussian processes\xspace}
\newcommand{\astar}{\alpha^{\star}}
\newcommand{\alphas}{{\alpha^{\star}}}
\newcommand{\betas}{\beta^{\star}}

\newcommand{\FF}{\mathcal{F}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\eq}[1]{\begin{align}#1\end{align}}
\newcommand{\sumn}[1]{\sum_{i=1}^n #1}


\theorembodyfont{\normalfont}
\newtheorem{problem}{Problem}[section]
\newtheorem{definition}{Definition}[section]

\title{Qualifying Exam Preparation I \\Methods}
\author{Xi Tan (tan19@purdue.edu)}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\newpage
\section{Simple Linear Regression}

\subsection{Model}
\begin{equation}
	Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\end{equation}
where $\epsilon_i \sim \NN(0,\sigma^2)$

\subsection{Estimated Regression Function}
\begin{eqnarray}
	b_1 & = & \rho_{XY} \cdot \frac{s_Y}{s_X} = \frac{\sum_{i=1}^n (X_i-\bar X) (Y_i-\bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2 } = \sum_{i=1}^n \left[\frac{X_i-\bar X}{\sum_{i=1}^n (X_i - \bar X)^2 }\right] Y_i \\
	b_0 & = & \bar Y - b_1 \bar X \\
	\hat \sigma^2 & = & \frac{MSE}{n-2} = \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{n-2}
\end{eqnarray}
Notice, $\sum(X_i-\bar X)^2 = \sum X_i^2 - n\bar X^2$.

The slope of the fitted line is equal to the correlation between $y$ and $x$ corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that it passes through the center of mass $(\bar x, \bar y)$ of the data points.

Another way of writing the estimated regression function is
\eq{
	\hat Y_i = \bar Y + b_1 (X_i - \bar X)	
}
Notice, $\bar Y$ and $b_1$ are uncorrelated (check it using the fact that $b_1 = \sumn k_i Y_i$).

\subsection{Properties of $k_i$}
\eq{
	k_i &= \frac{X_i - \bar X}{\sum_{i=1}^n (X_i-\bar X)^2}\\
	\sumn k_i &= 0\\
	\sumn k_i X_i &= 1\\
	\sum k_i^2 &= \frac{1}{\sumn(X_i - \bar X)^2}
}

The second and third identities hold as a requirement for the unbiasness, since $$E(b_1) = E\left(\sum k_i Y_i\right)= E\left(\sum k_i (\beta_0+\beta_1 X_i)\right) = E\left(k_i \sum\beta_0 + \beta_1 \sum k_i X_i \right) = \beta_1$$ requires $\sum k_i = 0$  and $\sum X_i k_i = 1$.
The fourth identity ensures the attainment of the minimum variance.


\subsection{Properties of $e_i$}
\begin{eqnarray}
	e_i & = & Y_i - \hat Y_i \\
	\sum e_i & = & 0 \\
	\sum X_i e_i & = & 0 \\
	\sum \hat Y_i e_i & = & 0
\end{eqnarray}

\subsection{Properties of $b_1$ and $b_0$}
\begin{eqnarray}
	b_1 &\sim& \NN \left(\beta_1,\sigma^2 \left[\frac{1}{\sum(X_i-\bar X)^2}\right]\right) \\
	b_0 &\sim& \NN \left(\beta_0,\sigma^2 \left[\frac{1}{n} + \frac{{\bar X}^2}{\sum(X_i-\bar X)^2}\right] \right)
\end{eqnarray}
where $\sigma^2$ can be estimated by the MSE, i.e., $\hat \sigma^2 = \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{n-2}$

\subsection{Inference About $b_1$ and $b_0$}
The confidence interval for $b_1$, with confidence level $\alpha$ is
\begin{equation}
	b_1 \pm t(1-\alpha/2;n-2)s\{b_1\}\\
\end{equation}
or
\begin{equation}
	b_1 \mp t(\alpha/2;n-2)s\{b_1\}\\
\end{equation}
Similarly, the confidence interval for $b_0$, with confidence level $\alpha$ is
\begin{equation}
	b_0 \pm t(1-\alpha/2;n-2)s\{b_0\}\\
\end{equation}
or
\begin{equation}
	b_0 \mp t(\alpha/2;n-2)s\{b_0\}\\
\end{equation}

\newpage
\begin{table}[h]
\centering
\begin{tabular}{c || c c c }
\hline
& {Estimate} & {Expectation} & {Variance} \\
\hline
\\
$Y_i$& $\hat Y_i$ & $\beta_0+\beta_1 X_i$ & $\sigma^2$\\
\\
\hline
\\
$b_1$& $\frac{\sum (X_i-\bar X) Y_i}{\sum (X_i-\bar X)^2}$ & $\beta_1$ & $\sigma^2 \cdot \frac{1}{\sum (X_i-\bar X)^2}$\\
\\
\hline
\\
$b_0$& $\bar Y - b_1 \bar X$ & $\beta_0$ & $\sigma^2 \cdot \left[\frac{1}{n} + \frac{{\bar X}^2}{\sum (X_i-\bar X)^2}\right]$\\
\\
\hline
\\
$\hat Y_h$& $\bar Y + b_1 (X_h-\bar X)$ & $\beta_0+\beta_1 X_h$ & $\sigma^2 \cdot \left[\frac{1}{n} + \frac{(X_h-\bar X)^2}{\sum (X_i-\bar X)^2}\right]$\\
\\
\hline
\\
$\hat Y_{h(new)}$& $\bar Y + b_1 (X_h-\bar X)$ & $\beta_0+\beta_1 X_h$ & $\sigma^2 \cdot \left[1 + \frac{1}{n} + \frac{(X_h-\bar X)^2}{\sum (X_i-\bar X)^2}\right]$\\
\\
\hline
\\
$\hat Y_{h(new_m)}$& $\bar Y + b_1 (X_h-\bar X)$ & $\beta_0+\beta_1 X_h$ & $\sigma^2 \cdot \left[\frac{1}{m} + \frac{1}{n} + \frac{(X_h-\bar X)^2}{\sum (X_i-\bar X)^2}\right]$\\
\\
\hline
\\
$e_i$& $Y_i-\hat Y_i$ & $0$ & $1-h_{ii}$\\
\\
\hline
\end{tabular}
\caption{Simple Linear Regression}
\end{table}

In particular, when $X_h = 0$ we obtain the formulas for $b_0$, and when $X_h - \bar X = 1$ we obtain the formulas for $b_1$.

\subsection{ANOVA of Simple Linear Regression Model}
\eq{
	SSTO &= SSR + SSE\\
	\sumn (Y_i - \hat Y_i) &= \sumn (\hat Y_i - \bar y) + \sumn (\bar y - \hat Y_i)
}
SSR can also be computed as $SSR = b_1^2 \sumn (X_i - \bar X)$, so given the same ``distribution'' of $X$, the steeper the slope of the regression line, the higher the SSR, and hence the better fit of the model.

To test $H_0: \beta_1 = 0$, we use $F = \frac{SSR}{SSE}$. There is equivalence between an $F$ test and a $t$ test: $[t(1-\alpha/2,n-2)]^2 = F(1-\alpha,n-2)$.








\section{Survivial Analysis}
\begin{eqnarray}
	S(t) &=& \exp \left[-\int_0^t \lambda(u) du \right] \\
	L(\lambda) &=& \prod_{i=1}^n [\lambda(t_i)]^{\delta_i} [S(t_i)]^{1-\delta_i}
\end{eqnarray}
where $S(t)$ is the survival function, and $\lambda(t)$ is the hazard function.

\begin{table}[h]
\centering
\begin{tabular}{c || c c c }
\hline
& {Estimate} & {Standard Error} & {NOTE} \\
\hline
\\
$S$& $\hat{S(t)} = \prod \frac{n_j-d_j}{n_j}$ & $\hat{S(t)} \sqrt{\sum \frac{d_i}{n_j(n_j-d_j)}}$ &\\
\\
\hline
\\
$\Lambda$& $-\log \hat{S(t)}$ & $\sqrt{\sum \frac{d_i}{n_j(n_j-d_j)}}$& \\
\\
\hline
\\
$\lambda$& $\frac{\sum \delta_i}{\sum (X_i-V_i)}$ & $\frac{\hat \lambda}{\sqrt{\sum \delta_i}}$\\
\\
\hline
\end{tabular}
\caption{Survival Analysis}
\end{table}


\section{Exponential Family}
\begin{eqnarray}
	f(y|\theta,\phi) & = & \exp \left\{ \frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi) \right\} \\
	E(y) & = & b'(\theta) \\
	Var(y) & = & b''(\theta)a(\phi)
\end{eqnarray}

\end{document} 






