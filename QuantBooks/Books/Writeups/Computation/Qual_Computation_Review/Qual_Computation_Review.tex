\documentclass{report}
\usepackage{amssymb,amsmath}
\usepackage[thmmarks,amsmath]{ntheorem}

\usepackage{hyperref}

\title{Qualifying Exam Preparation IV \\Computational Statistics}
\author{Xi Tan (tan19@purdue.edu)}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

\chapter*{Preface}
Textbook: Computational Statistics by G. H. Givens and J. A. Hoeting (GH)

TBD

\part{STAT598G Topics}

\chapter{Basic Computer Knowledge}
\section{Computer Organization, File Systems, Process Management, I/O Devices, Basic Linux Commands}
\section{Programming Languages, Interpreters and Compilers}

\chapter{C \& R Programming Languages}
\section{Basic Variable Types and Scope}
\section{Control Flows: if-else, for/while Loops}
\section{Functions}
\section{I/O Access}
\section{Pointers and Dynamic Memory allocation in C}
\section{Underflow and Overflow}
\section{Vector, Matrix and Distributions in R}
\section{Graphics in R; Calling C from R}

\chapter{Algorithm and Data Structures}
\section{Big-O, Small-o, Theta Notations; Time Complexity Analysis}
\section{Recursion and Divide \& Conquer}
\section{Dynamic Programming}
\section{Searching (Linear, Binary) and Sorting (Insertion Sort, Bubble Sort, Merge Sort)}
\section{Arrays, Linked Lists, Trees, Hash Tables and Related Operations}

\chapter{Statistical applications}
\section{Nonlinear Optimization : Gradient Descent, Coordinate Descent, Newton's Method, Line Search}
\section{Sampling: Generating Uniform Random Numbers, Transformation Method, Importance Sampling, Acceptance Sampling}
\section{Hidden Markov Models: Basic Definitions, Viterby Algorithm and Computing Forward and Backward Probabilities}
\section{Markov Random Fields, the Hammersley-Clifford Theorem, the Elimination Algorithm}
\section{Bootstrap}


\part{STAT598D Topics}

\chapter{Optimization}
\section{Unconstrained Optimization}
\subsection{Univariate Problems (Bisection, Newton)}
\subsubsection{Bisection Method}
Suppose $g'$ is continuous on $[a_0,b_0]$ and $g'(a_0)g'(b_0) \le 0$, then the Intermediate Value Theorem implies that there exists at least one $x^*$ for which $g'(x^*) = 0$ and hence $x^*$ is a local optimum of $g$. To find this local optimum, the Bisection Method systematically halves the interval at each iteration, by checking the product of $g'$.

The updating equations are
\begin{align}
[a_{t+1},b_{t+1}] =
	\begin{cases}
		[a_t,x^{(t)}], & \mbox{if } g'(a_t)g'(x^{t}) \le 0 \\
		[x^{(t)},b_t], & \mbox{if } g'(a_t)g'(x^{t}) > 0
	\end{cases}
\end{align}
and $x^{t+1} = \frac{a_{t+1}+b_{t+1}}{2}$.

\subsubsection{Newton's Method}
Suppose $g$ twice differentiable. At iteration $t$, Newton's method approximates $g'(x^*)$ by the linear Taylor series expansion:
\begin{align}
	g'(x^*) = g'(x^{(t)}) + (x^*-x^{(t)})(g''(x^{(t)}))
\end{align}
which gives us
\begin{align}
	x^* = x - \frac{g'(x^{(t)})}{g''(x^{(t)})}
\end{align}

\section{Quasi-Newton Methods}
\section{Simulated annealing}

\chapter{The EM-type Algorithms}
\section{The EM algorithm}
\section{The ECM and ECME algorithms}
\section{The PX-EM algorithm}
\section{Computing observed Fisher information matrix}

\chapter{Integration}
\section{Numerical integration}
\section{Random number generation}
\section{Simulation}
\section{Monte Carlo integration}

\chapter{Markov chain Monte Carlo (MCMC) methods}
\section{The Data Augmentation algorithm}
\section{The Gibbs sampler}
\section{The Metropolis-Hastings algorithm}
\section{Data Fusion and Particle Filter (Sequential MCMC)}
\section{Reversible jump MCMC}
\section{Convergence Diagnostics}




\end{document} 






