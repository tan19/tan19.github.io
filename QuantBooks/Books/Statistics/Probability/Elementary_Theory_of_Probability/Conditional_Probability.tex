\chapter{Conditional Probability}
\begin{deff}
	If $P(F) > 0$, then
	\begin{equation}
		P(E|F) = \frac{P(E,F)}{P(F)}
	\end{equation}
	$P(E|F)$ is called the conditional probability of $E$ given $F$. Conditional probability agrees with Definition \ref{def:probability}, and should be treated in the same way.
\end{deff}

\begin{deff}
	The multiplication rule
	\begin{equation}
		P(E_1, E_2, \ldots, E_n) = P(E_1) P(E_2|E_1) \ldots P(E_n|E_1, \ldots, E_{n-1})
	\end{equation}
\end{deff}

\begin{deff}
	Bayes' Formula
	\begin{equation}
		P(A_i|B) = \frac{P(B|A_i)P(A_i)}{P(B)}
	\end{equation}
	where $P(A_i)$ is sometimes called the prior distribution, $P(B|A_i)$ the likelihood function, and $P(A_i|B)$ the posterior distribution. The partition function $P(B)$ can be computed using the law of total probability
	\begin{equation}
		P(B) = \sum^n_{j=1}P(B|A_j)P(A_j)
	\end{equation}Â¥
\end{deff}

\begin{deff}
	Two events $E$ and $F$ are said to be {\em{independent}} if	
	\begin{equation}
		P(E,F) = P(E)P(F)
	\end{equation}
	A set of events are independent if every finite subset of these events is independent.
\end{deff}

\section{Conditional Probability}
\section{Conditional Expectation}
\section{Conditional Independence}