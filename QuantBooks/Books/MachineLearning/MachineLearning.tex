\documentclass{book}
\input{/Users/tan19/Dropbox/LaTeXMacros.tex}

\begin{document}
\title{Machine Learning}
\author{Xi Tan (xtan3.1415926@gmail.com)}

\maketitle
\tableofcontents
\input{../preface}
\addcontentsline{toc}{chapter}{Preface}


\chapter{Introduction}
There are two main types of Machine Learning problems: the {\bf{predictive}} or {\bf{supervised learning}}, and the {\bf{descriptive}} or {\bf{unsupervised learning}} \footnote{It is also called {\bf{Knowledge Discovery}} in the Data Mining literature}.

For supervised learning, there are two subtypes: the {\bf{classification}} problem, where the response variable is categorical (either ordinal or nomial); and the {\bf{regression}} problem, where the response variable is numerical (either discrete or continuous).

For unsupervised learning, it is also called {\bf{density estimation}} in Statistics literature. Essentially we want to build models of the form $p(\x_i|\theta)$, and supervised learning can be seen as to build models of the form $p(y_i|\x_i,\theta)$, which is a problem of conditional density estimation. \footnote{We see from the formulation that, unsupervised learning usually involves multivariate probability models while supervised learning usually involves univariate probability models.}


\section{Some Distinctions}
\subsection{Machine Learning v.s. Statistical Learning}
{\bf{Machine Learning}} focuses more on high dimension low noise situations, and performance and efficiency are the main concerns. {\bf{Statistical Learning}} focuses more on low dimension high noise situations, interpretation and statistical inference are the main concerns.
\subsection{Parametric v.s. Non-parametric Models}
A parametric model has a fixed {\em{finite}} number of parameters, while the number of parameters in a non-parametric model grows with the amount of training data. A model with infinite number of parameters is usually non-parametric.

\chapter{A Brief History of Machine Learning}
The perceptron model was invented in 1957, and it generated over optimistic view for AI during 1960s. After Marvin Minsky pointed out the limitation of this model in expressing complex functions, researchers stopped pursuing this model for the next decade.

In 1970s, the machine learning field was dormant, when expert systems became the mainstream approach in AI.  The revival of machine learning came in mid-1980s, when the decision tree model was invented and distributed as software. It is also in mid 1980s multi-layer neural networks were invented, With enough hidden layers, a neural network can express any function, thus overcoming the limitation of perceptron. We see a revival of the neural network study.

Around 1995, SVM was proposed and have become quickly adopted.

After year 2000, Logistic regression was rediscovered and re-designed for large scale machine learning problems . In the ten years following 2003, logistic regression has attracted a lot of research work.

We discussed the development of 4 major machine learning methods. There are other method developed in parallel, but see declining use today in the machine field: Naive Bayes, Bayesian networks, and Maximum Entropy classifier (most used in natural language processing). 

In addition to the individual methods, we have seen the invention of ensemble learning, where several classifiers are used together, and its wide adoption today. 


\url{http://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence}

1950s-1960s: the Perceptron Model
1970s-1980s: the Expert Systems
mid 1980s: Multi-layer Neural Networks
1995: SVM
2000s: Logistic Regression Rediscovered



\chapter{Regression v.s. Classification}
Consider a supervised learning problem in which we wish to approximate an unknown target function $f: \XX \to \YY$, or equivalently $P(Y|X)$. One way to learn $P(Y|X)$ is to use the training data to estimate $P(X|Y)$ and $P(Y)$, and then use Bayes rule to determine $P(Y|X)$.
\chapter{Parametric v.s. Nonparametric Models}
\chapter{Decision Trees}

\chapter{Clustering}
\chapter{Dimension Reduction}






\input{Chapters/Graphical_Models}
\chapter{Neural Networks}
\chapter{Kernel Methods}
\section{Support Vector Machines (SVM)}
\section{Gaussian Processes (GP)}

\chapter{Statistical Learning Theory}
Statistical learning theory studies the properties, in particular error-bounds, of learning algorithms in a statistical framework.

The No Free Lunch theorem says, if no assumptions about how training data are related to the testing data, prediction is impossible; furthermore, if no assumptions about the data to be expected, generalization is impossible.

Simply means we need to make the assumption that there is a stationary distribution of data.

\deff{
	Suppose $f: \RRR \to \RRR_+$ and $g: \RRR \to \RRR_+$, we write
	\begin{align}
		f = O(g)		
	\end{align}	
	if there exists $x_0, \alpha \in \RRR_+$ such that for all $x > x_0$ we have $f(x) \le \alpha g(x)$. Replacing $\le$ with $\ge$, we write $f = \Omega(g)$.
}

\deff{
	Suppose $f: \RRR \to \RRR_+$ and $g: \RRR \to \RRR_+$, we write
	\begin{align}
		f = o(g)
	\end{align}	
	if for every $\alpha > 0$ there exists $x_0$ such that for all $x > x_0$ we have $f(x) \le \alpha g(x)$. Replacing $\le$ with $\ge$, we write $f = \omega(g)$.
}

\deff{
	If $f = O(g)$ and $f = \Omega(g)$, then we write $f = \Theta(g)$.
}

\deff{
	We write $f = \tilde O(g)$ if there exists $k \in \NNN$ such that $f(x) = O(g(x)\log^k(g(x)))$.
}

\chapter{Latent Dirichlet allocation (LDA)}
\chapter{Principle Component Analysis (PCA)}
\chapter{Linear Discriminant Analysis (LDA)}

\chapter{Expectation Maximization (EM)}
\section{The EM algorithm}
\section{The ECM and ECME algorithms}
\section{The PX-EM algorithm}

\chapter{Expectation Propagation (EP)}


\chapter{Markov Chain Monte Carlo Methods}
\section{Introduction}
This note is based on Peter Orbanz's BNP notes:
\vspace*{5mm}
\\
\url{http://people.stat.sc.edu/hansont/stat740/MCMC.pdf}

\section{Notation}
Bold upper case letters represent matrices, e.g., $\X, \Y, \Z, \bTheta$. Bold lower case letters represent vector-valued random variables and their realizations (we do not distinguish between the two), e.g., $\x, \y, \z, \btheta$. Curly upper case letters represent spaces (i.e., possible values) of random variables, e.g., $\XX, \YY, \ZZ, \Theta$.

\section{Introduction}
Markov chain Monte Carlo (MCMC) methods can be used to draw random samples from a target distribution $p$. It is particularly useful in Bayesian data analysis, due to the difficulties of evaluating the denominator in the Bayes' formula, a.k.a. the partition function.

\begin{enumerate}
\item A discrete-time, discrete-space Markov chain is $\dual{X}{0}, \dual{X}{1}, \dots$ where $\dual{X}{t}$ obeys the Markov property that
\begin{align}
P\left[\dual{X}{t} \bigg| \dual{x}{0},\dots,\dual{x}{t-1}\right] = P\left[\dual{X}{t} \bigg| \dual{x}{t-1} \right]
\end{align}
\item A Markov chain is {\em{irreducible}} if any state $j$ can be reached from any state $i$ in a finite number of steps for all $i$ and $j$.
\item A Markov chain is {\em{periodic}} if it can visit certain portions of the state space only at regularly spaced intervals.
\end{enumerate}

The MCMC sampling strategy is to construct an irreducible, aperiodic Markov chain for which the stationary distribution equals the target distribution $p$.

Suppose we want to draw samples from $p(x)$. The M-H algorithm proceeds as follows: Draw a candidate state, $x^*$, according to the proposal distribution $g(x^*|x)$, by computing the acceptance probability
\begin{align}
\alpha(x^*, x) = \min\left[1, a(x^*,x) = \frac{p(x^*)g(x|x^*)}{p(x)g(x^*|x)}\right].
\end{align}
where $a(x^*,x)$ is called the M-H ratio, and $\alpha(x^*,x)$ the probability of move. With {\em{probability of move}} $\alpha(x^*, x)$, set the new state, $x'$ to $x^*$. Otherwise, let $x'$ be the same as $x$. The intuition behind the probability of move is that, if the detailed balance condition is satisfied: $p(x)g(x^*|x) = p(x^*)g(x|x^*)$, then we are done, otherwise, the denominator $g(x^*|x)p(x)$ is proportional to the probability of moving from $x$ to $x^*$, if it is large then the numerator, which is proportional to the probability of moving from $x^*$ to $x$, then we should penalize it.

The sampled sequence may contain duplicated copies of data points, the frequency of which is used to correct the difference between the proposal distribution and the target one. A well chosen proposal distribution produces candidate values that efficiently cover the support of the target distribution.

\section{Independence Chains}
If we choose the proposal distribution to be
\begin{align}
g(x^*|x) = g(x^*)
\end{align}
then the M-H ratio is
\begin{align}
a(x^*, x) = \frac{p(x^*)g(x)}{p(x)g(x^*)}.
\end{align}

For example, in a Bayesian framework, if the target distribution is the posterior $p(\theta|\y)$, where $\y$ is the data. Then, if we choose the proposal distribution to be the prior $p(\theta)$
\begin{align}
g(\theta^*|\theta) = p(\theta^*)
\end{align}
then the M-H ratio is
\begin{align}
a(\theta^*, \theta | \y) &= \frac{p(\theta^*|\y)p(\theta)}{p(\theta|\y)p(\theta^*)} = \frac{p(\y|\theta^*)p(\theta^*)/p(\y)}{p(\y|\theta)p(\theta)/p(\y)}\frac{p(\theta)}{p(\theta^*)} = \frac{p(\y|\theta^*)}{p(\y|\theta)}
\end{align}
So if the proposal distribution is the prior, the M-H ratio is the likelihood ratio.

\section{Random walk chains}
Let $x^*$ be generated by setting
\begin{align}
x^* = x + \epsilon,\quad \epsilon \sim h(\epsilon)
\end{align}
or equivalently,
\begin{align}
g(x^*|x) = h(x^* - x)
\end{align}
For example, $h$ can be the uniform, or the standard normal, or the Student's $t$ distribution.


\section{Gibbs sampler}
Suppose it is easy to sample from the univariate conditional distributions:
\begin{align}
x_i | \x_{-i} \sim f(x_i | \x_{-i})
\end{align}
then the basic Gibbs sampler can be described as follows:
\begin{enumerate}
\item Select starting values $\dual{x}{0}$ and set $t=0$.
\item Generate, in turn for $i = 1, \dots, n$:
\begin{align}
\dual{x_i}{t+1} | \x_{-i}^{(t)} \sim f\left(x_1 | \x_{-i}^{(t)}\right).
\end{align}
\item Increment $t$ and go to step 2.
\end{enumerate}
A hybrid MCMC may contain different types of samplers. For example, The M-H within Gibbs algorithm is typically useful when the univariate conditional density for one or more elements is not available in closed form.

\section{Test for Convergence}
\begin{enumerate}
\item Burn-in.
\item Run multiple chains, and if the within- and between-chain behaviors are similar, suggests that the chains are stationary. Gelman-Rubin statistic.
\item Plot samples against time, or log-likelihood against time.
\item Autocorrelation function (ACF) plot: lag versus correlation. Slow decay suggests poor mixing.
\item Re-parameterize the model may help.
\item Burn-in should be about $5000$ iterations, chain lengths should be about $100$ times the burn-in.
\item Standard error should be less than $5\%$ of the standard deviation.
\end{enumerate}

\section{How it is used}
Marginalization: just ignore others. Mean and variance: use samples. Probability estimates: estimated by the frequencies. Standard error of estimates: batch runs to obtain estimates and compute mean and standard error (divided by the square root of batch size). Density: kernel density or simply histogram.

\section{Advanced MCMC methods}
Slice sampling and other auxiliary variable methods, reversible jump MCMC, perfect sampling, Hit-and-run (choose a direction and then a distance to run), multi-try (choose from a set of candidates), Langevin M-H (random walk with drift) and etc.

\subsection{Slice sampling}
Introduce an auxilary varialbe $u$, and if we can sample from $f(x,u) = f(x)f(u|x)$ then dropping $u$ and retain $x$ as desired. The slice sampling works as follows:
\begin{align}
\dual{u}{t+1} | \dual{x}{t} &\sim \text{Unif}\left(0, f\left(\dual{x}{t}\right)\right)\\
\dual{x}{t+1} | \dual{u}{t+1} &\sim \text{Unif}\left(x: f(x) \ge \dual{u}{t+1}\right)
\end{align}
It is particularly useful for multi-modal problems (but not for high dimensional ones).

\subsection{Reversible Jump MCMC}
RJMCMC is suitable for nonparametric models where model dimensions change. The key is to use auxiliary variables to match the dimensions.

\section{Introduction}
\subsection{Sampling Methods in General}
\subsubsection{Inverse Transform Sampling}
\subsubsection{Importance Sampling}
\subsection{Rejection Sampling}
Suppose we want to sample from $P(X)$ by utilizing a {\em{proposal distribution}} $Q(X)$, from which we can take samples easier. Let $C = \sup \left\{\frac{P(x)}{Q(x)}, \forall x\right\}$, so $\frac{P(x)}{CQ(x)} \le 1, \forall x$.

We propose a new value $x'$ from $Q(X)$ and accept this new value with probability
\begin{align}
	A(x'|x) = \frac{P(x')}{CQ(x')} \label{rejection sampling ratio}
\end{align}
This is called the {\em{rejection sampling}} method.

\rmk{
	If we plug in Equation \ref{rejection sampling ratio} into Equation \ref{detailed balance equation}, then
	\begin{align}
			LHS = P(x)Q(x'|x)\frac{P(x')}{CQ(x')} = P(x)Q(x')\frac{P(x')}{CQ(x')} = \frac{1}{C}P(x)P(x')\\
			RHS = P(x')Q(x|x')\frac{P(x)}{CQ(x)} = P(x')Q(x)\frac{P(x)}{CQ(x)} = \frac{1}{C}P(x')P(x)
	\end{align}
	which shows that the rejection sampling ratio in Equation \ref{rejection sampling ratio} is a special solution to the detailed balance equation.
}


\section{Markov Chains}
\deff{
  A \emph{Markov chain} is a collection of random variables $\{X^{(0)},X^{(1)},X^{(2)},\cdots\}$, satisfying the Markov property
  \begin{align}
    P(X^{(n+1)} \vert X^{(n)}, \cdots, X^{(0)}) = P(X^{(n+1)} \vert X^{(n)})
  \end{align}
}

\deff{
  A distribution over the states of a homogeneous\footnote{The transition matrix is invariant of time.} Markov chain is \emph{invariant} (or \emph{stationary}) with respect to transition probabilities $T$ if
  \begin{align}
    \mathbf{\pi} = \mathbf{\pi} T
  \end{align}
}
A Markov chain can have more than one invariant distribution. If $T$ is the identity matrix, for example, then any distribution is invariant.

We are interested in designing a Markov chain (its initial probabilities and transition matrix) for which the distribution we wish to sample from, given by $\mathbf{\pi}$, is invariant. Hence, if we run the chain for sufficient time, it will converge to $\mathbf{\pi}$, and then we can sample from it and compute the quantities desired.

\emph{Why do we need detailed balance?}
\deff{
  Often, we will use \emph{time reversible} homogeneous Markov chains that satisfy the more restrictive condition of \emph{detailed balance}:
  \begin{align}
    \pi(x)T(x,x') = \pi(x')T(x',x) \label{detailed balance definition}
  \end{align}
}

\rmk{
One might be tempted to conclude that the detailed balance condition always holds, since
\begin{align}
	\pi(x)T(x,x') = P(x)P(x'|x) = P(x',x)\\
	\pi(x')T(x',x) = P(x')P(x|x') = P(x,x')
\end{align}
and $P(x',x) = P(x,x')$.

The problem here is, $x$ and $x'$ denote different values, not different random variables. That is, in the argument of $\pi(\cdot)$ or in the first argument of $T(\cdot,\cdot)$, it is the value of the random variable $X_0$ (stochastic process at current time); and in the second argument of $T(\cdot,\cdot)$, it is the value of the random variable $X_1$ (stochastic process at the next time step). For example, we may have
\begin{align}
	\pi(X_0=1)T(X_0=1,X_1=2) = P(X_0=1)P(X_1=2|X_0=1) = P(X_1=2,X_0=1)
\end{align}
and
\begin{align}	
	\pi(X_0=2)T(X_0=2,X_1=1) = P(X_0=2)P(X_1=1|X_0=2) = P(X_1=1,X_0=2)
\end{align}
which are usually not the same. Here $x=1$ and $x'=2$.
}

Note, the detailed balance condition implies $\pi$ is an invariant distribution:
\begin{align}
	\sum_{i=1}^n \pi(x_i) T(x_i,x) = \sum_{i=1}^n \pi(x) T(x,x_i) = \pi(x) \sum_{i=1}^n T(x,x_i) = \pi(x)
\end{align}
It is possible for a distribution to be invariant without detailed balance holding. For example, the uniform distribution ($=1/3$) on the state space $\{0,1,2\}$ is invariant with respect to the homogeneous Markov chain with transition probabilities $T(0,1)=T(1,2)=T(2,0)=1$ and all others zero, but detailed balance does not hold.

\deff{
  A Markov chain is \emph{ergodic} (or \emph{irreducible}) if it is possible to go from every state to every state (not necessarily in one move).
}
\thm{
  Let $T$ be the transition matrix for a regular chain. Then as $n \to \infty$, the powers $T^n$ approach a limiting matrix $W$ with all rows the same vector $\w$. The vector $\w$ is a strictly positive probability vector (i.e., the components are all positive and they sum to one).
}

\section{Metropolis-Hastings Algorithm}
Suppose we can evaluate a distribution $P(X)$, but lack of information about how to directly draw samples from it. We wish to construct a Markov chain, utilizing the detailed balance condition (Equation \ref{detailed balance definition}), such that the induced invariant distribution is nothing but $P(X)$.

Assume we can take a new sample $x'$ from $Q(x'|x)$, a known {\em{proposal distribution}} conditioned on the current state $x$ of the Markov chain, and then we accept this new value $x'$ based on a to-be-determined probability $A(x'|x)$. We define a Markov chain based on this procedure,
\begin{align}
	T(x,x') = Q(x'|x)A(x'|x)
\end{align}

If this Markov chain further satisfies the detailed balance condition (Equation \ref{detailed balance definition}),
\begin{align}
	P(x)Q(x'|x)A(x'|x) = P(x')Q(x|x')A(x|x') \label{detailed balance equation}
\end{align}
then we can take
\begin{align}
	A(x'|x) = \min \left\{ \frac{P(x')Q(x|x')}{P(x)Q(x'|x)}, 1 \right\} \label{hastings ratio}
\end{align}
The ratio in the ``$\min$'' function is known as the ``Hastings ratio'' (or acceptance ratio).

Now, running the chain for some sufficient time, we would obtain samples from the desired distribution $P(X)$, which is designed to be the same as the induced invariant distribution.

\rmk{
	The $A(x'|x)$ defined above satisfies the detailed balance condition, since
	\begin{align}
		P(x)Q(x'|x)A(x'|x) = P(x)Q(x'|x) \min \left\{ \frac{P(x')Q(x|x')}{P(x)Q(x'|x)}, 1 \right\} = \min \left\{ P(x')Q(x|x'), P(x)Q(x'|x) \right\}\\
		P(x')Q(x|x')A(x|x') = P(x')Q(x|x') \min \left\{ \frac{P(x)Q(x'|x)}{P(x')Q(x|x')}, 1 \right\} = \min \left\{ P(x)Q(x'|x), P(x')Q(x|x') \right\}
	\end{align}
	and $\min \left\{ P(x')Q(x|x'), P(x)Q(x'|x) \right\} = \min \left\{ P(x)Q(x'|x), P(x')Q(x|x') \right\}$.
}

\subsection{Metropolis Algorithm}
In the Hastings ratio, if the proposal distribution is symmetric $Q(x|x') = Q(x'|x)$, such as a Gaussian distribution, then Equation \ref{hastings ratio} becomes
\begin{align}
	A(x'|x) = \min \left\{ \frac{P(x')Q(x|x')}{P(x)Q(x'|x)}, 1 \right\} = \min \left\{ \frac{P(x')}{P(x)}, 1 \right\}
\end{align}
this special case is called the ``Metropolis Algorithm''.

\subsection{Gibbs Sampling}
Suppose we wish to sample a random vector $\X = (X_1,\cdots,X_n)$, and its full conditional distribution $Q(X_i|\X_{-i})$ is known, where $\X_{-i} = (X_1,\cdots,X_{i-1},X_{i+1},X_n)$. Then if we sample $X_i$ component-wise from $Q(\x'|\x) = Q(x_i'|\x_{-i})$, and bearing in mind that $\x'_{-i} = \x_{-i}$ when sample $x_i$, the Hastings ratio becomes,
\begin{align}
	\frac{P(\x')Q(\x|\x')}{P(\x)Q(\x'|\x)} &= \frac{P(x_i'|\x'_{-i})P(\x'_{-i})Q(x_i|\x'_{-i})}{P(x_i|\x_{-i})P(\x_{-i})Q(x_i'|\x_{-i})}\\
	&= \frac{P(x_i'|\x_{-i})P(\x_{-i})Q(x_i|\x_{-i})}{P(x_i|\x_{-i})P(\x_{-i})Q(x_i'|\x_{-i})}\\	
	&= 1
\end{align}

\subsection{Collapsed Gibbs Sampling}

\subsection{Metropolis-Within-Gibbs}
If not all full conditional probabilities are known or can be easily sampled from, we can sample those random variables with known conditional probabilities using the Gibbs algorithm, and others using Metropolis-Hastings algorithm. This is called {\em{Metropolis-Within-Gibbs}}.

\section{Slice Sampling}
\subsection{Elliptical Slice Sampling}

\section{Split-Merge Sampling}

\section{Hamiltonian Monte Carlo}

\section{Data Fusion and Particle Filter (Sequential MCMC)}
\section{Reversible jump MCMC}
\section{Convergence Diagnostics}



\chapter{Bayesian Nonparametrics}
The concept of functions can be generalized to that of algorithms, which describe procedures, with loops and conditional tests, of how to generate output from input.

A draw from a finite dimensional Gaussian distribution is a real number, while a real-valued function can be considered as a sequence of (uncountably) infinite number of real numbers. A Gaussian Process (GP) is an infinite dimensional generalization of a Gaussian distribution. It defines a prior over real-valued functions, and a sample of it is a particular example of such functions.

A draw from a finite dimensional Dirichlet distribution is a (discrete) probability measure. A Dirichlet Process (DP) is an infinite dimensional generalization of a Dirichlet distribution. It defines a prior over probability measures, and a sample of it is a probability measure.  Distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters, thus the classification as a nonparametric model.


Note, that we do not have a measurement of the function, as in the GP case but a sample of the true probability measure; this is the main difference between GP and DP.

\section{Introduction}
This note is based on Peter Orbanz's BNP notes:
\vspace*{5mm}
\\
\url{http://stat.columbia.edu/~porbanz/npb-tutorial.html}

\section{Notation}
Bold upper case letters represent matrices, e.g., $\X, \Y, \Z, \bTheta$. Bold lower case letters represent vector-valued random variables and their realizations (we do not distinguish between the two), e.g., $\x, \y, \z, \btheta$. Curly upper case letters represent spaces (i.e., possible values) of random variables, e.g., $\XX, \YY, \ZZ, \Theta$.

\section{Terminology}
\subsection{Parametric and nonparametric models}
In a set of probability spaces $\{(\YY, \FF, \PP_\Theta)\}$, a {\em{statistical model}} $\MM$ on a sample space $\YY$ is {\underline{a set of probability measures}} $\PP_\Theta$ on $\YY$. If we write $PM(\YY)$ for the space of all probability measure on $\YY$, a model is a subset $\MM \subset PM(\YY)$. Every element of $\MM$ has a one-to-one mapping (hence the model is {\em{identifiable}}) with its parameter $\btheta$ with values in a parameter space $\Theta$, that is,
\begin{align}
\MM(\y) = \{P_\btheta(\y) | \btheta \in \Theta\},\quad \y \in \YY.
\end{align}
For example, a first order polynomial is a model, and a second order polynomial is another model. We can of course fit a model to the observed data, but {\em{model}} itself is an abstract concept, where the parameter values of a model need not be specified. We call a model {\em{parametric}} if $\Theta$ has finite dimension, and {\em{nonparametric}} if $\Theta$ has infinite dimension.

To formulate statistical problems, we assume that $n$ observations $\y_1,\dots,\y_n$ with values in $\YY$ are observed, which are drawn i.i.d. from a measure $P_\btheta$ in the model, i.e.,
\begin{align}
\y_1, \dots, \y_n \sim_{iid} P_\btheta \qquad \text{for some}~~ \btheta \in \Theta
\end{align}
The objective of statistical {\em{inference}} is then to draw conclusions about the value of $\btheta$ (and hence about the distribution $P_\btheta$ of the data) from the observations.


\subsection{Bayesian and Bayesian nonparametric models}
In Bayesian statistics, all parameters are considered as random variables. Hence under a Bayesian model, data are generated in two stages, i.e.,
\begin{align}
\btheta &\sim P(\btheta)\\
\y_1, \dots, \y_n ~|~ \btheta &\sim_{iid} P_\btheta(\y)
\end{align}
The objective is then to determine the {\em{posterior distribution}} -- the conditional distribution of $\btheta$ given the observed data,
\begin{align}
\pi(\btheta | \y_1, \dots, \y_n)
\end{align}
A {\em{Bayesian nonparametric}} model is a Bayesian model whose parameter space $\Theta$ has infinite dimension. To define a Bayesian nonparametric model, we have to define a prior $\pi$ on an infinite-dimensional space, which is a stochastic process with paths (i.e. realizations) in $\Theta$.

\section{Clustering and the Dirichlet process}
\subsection{Finite mixture models}
The basic assumption of a clustering problem is that each observation $\y_i$ belongs to a single cluster $k \in \{1,\cdots, K\}$, which has a cluster distribution
\begin{align}
P_k(\y_i | z_i = k)
\end{align}
where we have defined a latent variable $z_i$, indicating the cluster assignment of observation $\y_i$. Note that under the Bayesian framework, the latent variable $z_i$ itself has a distribution
\begin{align}
p_k^i \equiv P(z_i = k)
\end{align}

The marginal distribution of the observation $\y_i$ is then
\begin{align}
P(\y_i) = \sum_{k=1}^K P(z_i = k) P_k(\y_i | z_i = k)
\end{align}
A model of this form is called a {\em{finite mixture model}}.

\subsection{Bayesian mixture models}
Suppose we know there are $K$ clusters, we first sample the cluster parameters from some base measure:
\begin{align}
\btheta_1, \dots, \btheta_K \sim_{iid} G(\bbeta)
\end{align}
We then independently sample the latent cluster assignment vectors and the actual observations:
\begin{align}
(p_1^i, \dots, p_K^i) &\sim \text{Dirichlet}_K(\alpha)\\
z_i &\sim \text{Categorical}(p_1^i, \dots, p_K^i)\\
\y_i &\sim P_k(\y_i | \btheta_k, z_i = k)
\end{align}

\subsection{Dirichlet Process}
\deff{
	If $\alpha > 0$ and if $G$ is a probability measure on $\Omega_\phi$, the random discrete probability measure $\Theta$ generated by
	\begin{align}
	V_1, V_2, \dots \sim_{iid} \text{Beta}(1,\alpha)\\
	C_k = V_k\prod_{j=1}^{k-1}(1-V_k)\\
	\Phi_1, \Phi_2, \dots \sim_{iid} G
	\end{align}
	is called a {\em{Dirichlet process (DP)}} with base measure $G$ and concentration $\alpha$, and denote its law by $\text{DP}(\alpha, G)$.
}

\appendix
\chapter{Glossary}
\url{http://alumni.media.mit.edu/~tpminka/statlearn/glossary/}


- Model Evidence: Model evidence, or *marginal likelihood*, or *normalization constant*, is a likelihood function in which all parameter variables have been marginalized, usually denoted by $P(D|M)$, or $P(D)$ for short. For example, in polynomial regression, $M$ may denotes the degree of the regression function, and parameter variables are the coefficients for any given M.

- Filtering is the task of tracking the posterior distribution of the latent state varialbe in state space models.

\input{Chapters/QA}

\chapter{Useful Resources}
\section{Data Sets}
\section{Packages and Source Codes}
\section{Important Papers}



\begin{thebibliography}{100} % 100 is a random guess of the total number of %references  
\bibitem{Stewart} James Stewart {\em{Calsulus - Early Transcendentals}}. Cengage Learning, 2012
\bibitem{Rudin} Walter Rudin {\em{Principles of Mathematical Analysis}}. McGraw-Hill Companies, Inc., 1976.
\bibitem{Royden} H. L. Royden {\em{Real Analysis}}. Pearson Eduction, Inc., 1988.
\bibitem{Kreyszig} Erwin Kreyszig {\em{Introductory Functional Analysis with Applications}}. Wiley, 1989.
\bibitem{Folland} Gerald B. Folland {\em{Real Analysis: Modern Techniques and Their Applications}}. Wiley, 1999.
\bibitem{Torchinsky} Alberto Torchinsky {\em{Real Variables}}. Westview Press, 1995.

\bibitem{Wasserman} {\url{http://normaldeviate.wordpress.com/2012/11/17/what-is-bayesianfrequentist-inference/}}
\bibitem{Hochster} {\url{http://www.quora.com/What-is-the-difference-between-Bayesian-and-frequentist-statisticians}}
\bibitem{bayesian-inference advantage} {\url{http://www.bayesian-inference.com/advantagesbayesian}}
\bibitem{bayesian-inference likelihood} {\url{http://www.bayesian-inference.com/likelihood#likelihoodprinciple}}
\bibitem{Rossi} Rossi P, Allenby G, McCulloch R. {\emph{Bayesian Statistics and Marketing (pp. 4)}}. John Wiley \& Sons, 2005.
\bibitem{Efron 1978} Efron, Bradley. {\emph{Controversies in the Foundations of Statistics}}. The American Mathematical Monthly, Vol. 85, No. 4 (Apr., 1978), pp. 231-246.
\bibitem{Efron 2013} Efron, Bradley. {\emph{A 250-year Argument: Belief, Behavior, and the Bootstrap}}. Bull. Amer. Math. Soc. 50 (2013), 129-146.
\bibitem{quora CI} {\url{http://www.quora.com/Statistics-academic-discipline/What-is-a-confidence-interval-in-laymans-terms}}
\bibitem{quora diff} {\url{http://www.quora.com/What-is-the-difference-between-Bayesian-and-frequentist-statisticians}}
\bibitem{wiki} {\url{http://en.wikipedia.org/wiki/Confidence_interval#Meaning_and_interpretation}}

\bibitem{Minka} Thomas P. Minka. {\em{Old and New Matrix Algebra Useful for Statistics}}. December 28, 2000.
\bibitem{Wikepedia} \url{http://en.wikipedia.org/wiki/Matrix_calculus}. Accessed on \today
\bibitem{Searle} S. R. Searle and H. V. Henderson. {\em{A Primer on Differential Calculus for Vectors and Matrices}}. BU-1047-MB, 1993.
\bibitem{Nydick} Steven W. Nydick. {\em{A Different(ial) Way Matrix Derivatives Again}}. May 17, 2012.
\bibitem{Nydick} Steven W. Nydick. {\em{With(out) A Trace Matrix Derivatives the Easy Way}}. May 16, 2012.
\bibitem{Roweis} Sam Roweis. {\em{Matrix Identities}}. June 1999.
\bibitem{Tao} Terry Tao. {\em{Matrix identities as derivatives of determinant identities}}. January 13, 2013
\end{thebibliography}

\end{document}
