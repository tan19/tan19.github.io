\section{Special Square Matrices}
\section{Elementary Matrices}
There are three types of elementary matrices: {\color{red}{Row Switching, Row Multiplication, and Row Addition.}}

\begin{rmk}
	Left multiplication (pre-multiplication) by an elementary matrix represents elementary row operations, while right multiplication (post-multiplication) represents elementary column operations.
\end{rmk}

\begin{rmk}
	The inverse of elementary matrices has the same format as the orginal ones.
\end{rmk}

\section{Permutation  Matrices}
\begin{rmk}
	When a permutation matrix $P$ is multiplied with a matrix M from the left it will permute the rows of $M$, when $P$ is multiplied with $M$ from the right it will permute the columns of $M$.
\end{rmk}

\begin{rmk}
	The inverse of a permutation matrix is its transpose.
\end{rmk}

\section{Symmetric Matrices}

\section{Projection Matrices}
\begin{rmk}
	$P = A(A^TA)^{-1}A^T$, $P = \frac{aa^T}{\|a\|}$
\end{rmk}

\begin{rmk}
	$P^2=P$
\end{rmk}

\begin{rmk}
	Only two eigenvalues possible: 0 and 1. The corresponding eigenvectors form the kernel and range of $A$, respectively.
\end{rmk}

\begin{rmk}
	Projection is invertible.
\end{rmk}

\section{Normal Matrix}
\deff{
	A {\em{normal matrix}} is a square matrix which satisfies
	\begin{align}
		\A^T\A = \A\A^T
	\end{align}
}

\section{Orthogonal Matrices}
\deff{
	An {\em{orthogonal matrix}} ({\em{unitary}} for a complex matrix) is a normal matrix which further satisfies
	\begin{align}
		\A^T\A = \A\A^T = \I
	\end{align}
}
Or, alternatively,
%\rmk{An orthogonal matrix is a square matrix with orthonormal \smallmarginpar{orthogonal and unit vectors} columns.}

\begin{rmk}
	$Q^TQ = I$ even if $Q$ is rectangular (but then left-inverse).
\end{rmk}

\begin{rmk}
	Any permutation matrix $P$ is an orthogonal matrix.
\end{rmk}

\rmk{Orthogonal matrices can be categorized into either the reflection matrix $Ref(\theta)$ which has determinant 1, or the rotation matrix $Rot(\theta)$, which has determinant -1.}

\rmk{Geometrically, an orthogonal $Q$ is the product of a rotation and a reflection.}

\begin{rmk}
	Orthogonal matrix is invariant to 2-norm, that is, suppose $Q$ is an orthogonal matrix, and $x$ a vector, then
	\begin{align}
		\|Qx\| = \|x\|
	\end{align}
\end{rmk}

\begin{rmk}
	Projection matrices are usually not orthogonal, since they are not invariant to 2-norm.
\end{rmk}


\rmk{As a linear transformation, an orthogonal matrix preserves the dot product of vectors (therefore also norm and angle), and therefore acts as an isometry of Euclidean space, such as a rotation or reflection. In other words, it is a unitary transformation.}

\rmk{The product of two rotation matrices is a rotation matrix, and the product of two reflection matrices is also a rotation matrix. See figure~\ref{fig:rotation}.}
%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.4]{rotation}
%\caption{The product of two reflection matrices is a rotation matrix.}
%\label{fig:rotation}
%\end{figure}

\section{Positive Definite Matrices}
\deff{
	Let $\A$ be an $n \times n$ square matrix. $\A$ is said to be positive definite if
	\begin{align}
		\x^T \A \x > 0, ~~ \forall \x \ne \mathbf{0}
	\end{align}
}

%\thm{\marginnote{\dbend}
%	If $\A$ is positive definite, then
%	\begin{enumerate}
%		\item The diagonal elements of a positive definite matrix are positive.
%		\item All eigenvalues of $\A$ is positive.
%		\item Its determinant is positive.
%		\item It is nonsingular.
%	\end{enumerate}
%}
\noindent
\prof{
	The diagonal elements are positive because $a_{kk} = \e_k^T \A \e_k > 0$. The eigenvalues of an s.p.d. matrix are all positive is easy to prove by observing that $$0 < \x^T\A\x = \x^T\lambda\x = \lambda \|\x\|_2^2$$ The positivity of determinant can be shown by looking at the LDU decomposition. Finally, it is nonsingular because the determinant is nonzero.
}	
\deff{
	Let $\A$ be an $n \times n$ square matrix. A principal submatrix of $\A$ is obtained by selecting some rows and columns with the {\em{same}} index subset of $\{1, \cdots, n\}$.
}
\deff{
	Let $\A$ be an $n \times n$ square matrix. A {\em{leading}} principal submatrix of $\A$ is a principal submatrix of $\A$ with the index subset $\{1,\cdots,m\}$, for some $m \le n$.
}

%\thm{\marginnote{\dbend}
%	If $\A$ is positive definite then every principle submatrix is s.p.d..
%}
\prof{
	Suppose $\A_p$ of size $p$ is a principle submatrix of $\A$. Since $\A$ is positive definite, for any nonzero vector $\x$ we have $\x^T \A \x > 0$. Remove the corresponding coordinates of $\x$, same as those removed when creating the principle submatrix, and call it $\x_p$. Then the resulting vector $\x_p^T \A_p \x_p = \x^T \A \x > 0$.
}


\section{Numerical Linear Algebra Algorithms}
\section{Matrix Inverse: Binomial inverse theorem, Schur Complement, Blockwise Inversion}
\begin{rmk}
	$\mathbf{A}^{-1} = \begin{bmatrix}
a & b \\ c & d \\
\end{bmatrix}^{-1} =
\frac{1}{\det{\mathbf{A}}} \begin{bmatrix}
\,\,\,d & \!\!-b \\ -c & \,a \\
\end{bmatrix} =
\frac{1}{ad - bc} \begin{bmatrix}
\,\,\,d & \!\!-b \\ -c & \,a \\
\end{bmatrix}$
\end{rmk}

Usually, $|\A\B| \ne |\B\A|$. For example
\ex{
    \begin{align*}
        \begin{vmatrix}\left[\begin{matrix}1 ~~ 1\end{matrix}\right] \left[\begin{matrix}0 \\ 1\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}1\end{vmatrix} = 1
    \end{align*}

    \begin{align*}
        \begin{vmatrix}\left[\begin{matrix}0 \\ 1\end{matrix}\right] \left[\begin{matrix}1 ~~ 1\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\left[\begin{matrix}0 ~~ 0 \\ 1 ~~ 1\end{matrix}\right]\end{vmatrix} = 0
    \end{align*}
}

However, the {\bf{Sylvester's Determinant Theorem}} says, as long as $\A\B$ and $\B\A$ are both square matrices,
\begin{align}
    \begin{vmatrix}\I+\A\B\end{vmatrix} = \begin{vmatrix}\I+\B\A\end{vmatrix}
\end{align}

It is also not true in general that
\begin{align*}
    \begin{vmatrix}\left[\begin{matrix}\A ~~ \B \\ \C ~~  \D\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\A\D-\B\C\end{vmatrix}
\end{align*}
unless $\C$ and $\D$ are commutable, i.e., $\C\D = \D\C$. The general formula for block determinant is
\begin{align}
    \begin{vmatrix}\left[\begin{matrix}\A ~~ \B \\ \C ~~  \D\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\A\end{vmatrix}\begin{vmatrix}\D - \C\A^{-1}\B\end{vmatrix}
\end{align}
which is based on Schur complement.


\section{The $\A\x=\b$ Problem}
\section{Solving a Linear System of Equations}
\thm{
	If $A = \left[\begin{matrix}a & b\\ c & d\end{matrix}\right]$, then $A$ is invertible if $ad-bc\ne 0$, in which case $$A^{-1} = \frac{1}{ad-bc} \left[\begin{matrix}d & -b\\ -c & a\end{matrix}\right]$$
}
\begin{rmk}
	Three ways to solve a system of linear equations: by elimination, by determinants {\color{red}{(Cramer's Rule???)}}, or by matrix decomposition.
\end{rmk}

\begin{rmk}
	We prefer to use matrix decomposition to solve a linear system because
	\begin{enumerate}
		\item It takes $\mathcal{O}(n^3)$ to factorize, but once done it can be used to solve systems with different $\b$ (right hand side).
		\item It is numerically more stable than computing $\A^{-1}\b$.
		\item For a sparse matrix, the inverse may be dense and may hard to store in memory. Decomposition can overcome this problem.
	\end{enumerate}
\end{rmk}

\rmk{
	Cofactors and Minors. Laplaceâ€™s Theorem.
}

\begin{rmk}
	The computation of elimination is $\mathcal{O}(n^3)$, but can be (non-trivially) reduced to $\mathcal{O}(n^{\log_27})$.
\end{rmk}

\section{The Vector Spaces of a Matrix}
\begin{rmk}
	$Ax$ is a combination of the {\em{columns}} of $A$. $b^TA$ is a combination of the {\em{rows}} of $A$. Row picture can be seen as interchapter of (hyper-)planes. Column picture can be seen as combination of columns.
\end{rmk}

\begin{rmk}
	There are three different ways to look at matrix multiplication:
	\begin{enumerate}
		\item Each entry of $AB$ is the product of a row (of $A$) and a column (of $B$)
		\item Each {\em{column}} of $AB$ is the product of a matrix (of $A$) and a column (of $B$)
		\item Each {\em{row}} of $AB$ is the product of a row (of $A$) and a matrix (of $B$)
	\end{enumerate}
\end{rmk}

\begin{rmk}
	Column space is perpendicular to the left null space. Row space is perpendicular to the null space.
\end{rmk}




\section{The $\A\x=\lambda \x$ Problem}

\section{Matrix Decomposition}
\section{Decomposition related to solving $\A\x = \b$}
\subsection{LU Decomposition: Schur Complement}
Usually, $|\A\B| \ne |\B\A|$. For example
\ex{
    \begin{align*}
        \begin{vmatrix}\left[\begin{matrix}1 ~~ 1\end{matrix}\right] \left[\begin{matrix}0 \\ 1\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}1\end{vmatrix} = 1
    \end{align*}

    \begin{align*}
        \begin{vmatrix}\left[\begin{matrix}0 \\ 1\end{matrix}\right] \left[\begin{matrix}1 ~~ 1\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\left[\begin{matrix}0 ~~ 0 \\ 1 ~~ 1\end{matrix}\right]\end{vmatrix} = 0
    \end{align*}
}

However, the {\bf{Sylvester's Determinant Theorem}} says, as long as $\A\B$ and $\B\A$ are both square matrices,
\begin{align}
    \begin{vmatrix}\I+\A\B\end{vmatrix} = \begin{vmatrix}\I+\B\A\end{vmatrix}
\end{align}

It is also not true in general that
\begin{align*}
    \begin{vmatrix}\left[\begin{matrix}\A ~~ \B \\ \C ~~  \D\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\A\D-\B\C\end{vmatrix}
\end{align*}
unless $\C$ and $\D$ are commutable, i.e., $\C\D = \D\C$. The general formula for block determinant is
\begin{align}
    \begin{vmatrix}\left[\begin{matrix}\A ~~ \B \\ \C ~~  \D\end{matrix}\right]\end{vmatrix} = \begin{vmatrix}\A\end{vmatrix}\begin{vmatrix}\D - \C\A^{-1}\B\end{vmatrix}
\end{align}
which is based on Schur complement.

Now suppose we have a homogeneous linear system
\begin{align}
	\left[\begin{matrix}\A & \B \\ \C & \D \end{matrix}\right] \left(\begin{matrix} \x \\ \y \end{matrix}\right) = \left(\begin{matrix} \mathbf{0} \\ \mathbf{0} \end{matrix}\right)
\end{align}
To solve for $\y$, if $\A$ is nonsingular, we may multiply the first row by $-\C\A^{-1}$ and add to the second, and obtain
\begin{align}\label{eq:shurA}
	\left[\begin{matrix}\I & \mathbf{0} \\ -\C\A^{-1} & \I \end{matrix}\right] \left[\begin{matrix}\A & \B \\ \C & \D \end{matrix}\right] = \left[\begin{matrix}\A & \B \\ \mathbf{0} & \D-\C\A^{-1}\B \end{matrix}\right]	
\end{align}

\deff{
Suppose $\M$ is a square matrix $$\M = \left[\begin{matrix}\A & \B \\ \C & \D \end{matrix}\right]$$ and $\A$ nonsingular.
We denote \footnote{It is easy to remember if you multiply the submatrices clockwise.}
\begin{align}
	\M/\A = \D-\C\A^{-1}\B
\end{align}
and call it {\em{the Schur complement of $\A$ in $\M$}}, or {\em{the Schur complement of $\M$ relative to $\A$}}.

%\rmk{
%A very useful identity can be revealed from equation \ref{eq:shurA} \marginnote{\dbend}
%\begin{align}
%	\M = \left[\begin{matrix}\A & \B \\ \C & \D \end{matrix}\right] = \left[\begin{matrix}\I & \mathbf{0} \\ \C\A^{-1} & \I \end{matrix}\right] \left[\begin{matrix}\A & \mathbf{0} \\ \mathbf{0} & \D-\C\A^{-1}\B \end{matrix}\right] \left[\begin{matrix}\I & \A^{-1}\B \\ \mathbf{0} & \I \end{matrix}\right]
%\end{align}
%}which gives us the following identities
\thm{
\begin{align}
	det(\M) &= det(\M/\A) \cdot det(\A)\\
	rank(\M) &= rank(\M/\A) + rank(\A)
\end{align}
}
\rmk{
For a non-homogeneous system of linear equations $$\left[\begin{matrix} \A & \B \\ \C & \D \end{matrix}\right] \left(\begin{matrix} \x \\ \y \end{matrix}\right) = \left(\begin{matrix} \u \\ \v \end{matrix}\right)$$ We may use Schur complements to write the solution as
\begin{align}
	\x = (\M/\D)^{-1}(\u-\B\D^{-1}\v)\\
	\y = (\M/\A)^{-1}(\v-\C\A^{-1}\u)
\end{align}
}
\thm{
	If $\M$ is a positive-definite symmetric matrix, then so is the Schur complement of $\D$ in $\M$.
}
\subsection{LDU Decomposition}
The LDU decomposition can be viewed as the matrix form of Gaussian elimination. It is used to find the inverse of a matrix, {\bf{or computing the determinant of a matrix}}.

\begin{rmk}
	The triangular factorization can be written $A=LDU$, where $L$ and $U$  have $1's$ on the diagonal and $D$ is the diagonal matrix of pivots.
\end{rmk}
\subsection{Rank Decomposition}
\subsection{Cholesky Decomposition}
\deff{
	The Cholesky decomposition of an s.p.d. matrix $\A$ is of the form
	\begin{align}
		\A = \L\L^*
	\end{align}
	where $\L$ is a lower triangular matrix, with {\em{real and positive diagonal elements}}.
}

\deff{
	The Cholesky decomposition of a s.p.d. matrix $\A$ is of the form
	\begin{align}
		\A = \L\L^T
	\end{align}
	where $\L$ is a lower triangular matrix, with {\em{real and positive diagonal elements}}.
}

Cholesky decomposition is unique. If $\A$ is symmetric semi-positive definite, it still has a decomposition of the form $\A = \L\L^*$, although may not be unique, if the diagonal entries of $\L$ are allowed to be zero. A closely related variant of the classical Cholesky decomposition is the {\em{$LDL^T$ decomposition}}:
\begin{align}
	\A = \L\D\L^T = (\L\D^{\frac{1}{2}}) ({\D^{\frac{1}{2}}}^T\L^T) = (\L\D^{\frac{1}{2}}) (\L\D^{\frac{1}{2}})^T
\end{align}
where the diagonal entries of $\L$ are all ones.


\subsection{QR Decomposition: Givens Rotation, Householder Transformation}
Any square matrix $\A$ may be decomposed as
\begin{align}
	\A = \Q\R
\end{align}
where $\Q$ is an orthogonal matrix, and $\R$ an upper triangular matrix. This is called the QR decomposition. It is essentially a change of basis process, and can be obtained by using the Gram-Schmidt process.


\section{Decomposition related to solving $\A\x = \lambda\x$}
\subsection{Eigendecomposition}
Suppose a square matrix $\M$ of order $n$ is diagonalizable, i.e., it has $n$ linearly independent eigenvectors, then since
\begin{align}
	\M\Q = \Q\bLambda
\end{align}
where the columns of $\Q$ are eigenvectors of $\M$ (hence invertible), and $\bLambda$ is a diagonal matrix with eigenvalues of $\M$ as entries. Then we have
\begin{align}
	\M = \Q\bLambda\Q^{-1}
\end{align}


\subsection{Jordan Decomposition}
\subsection{Schur Decomposition}
\subsection{Singular Value Decomposition (SVD)}
\subsection{QZ Decomposition}

\section{Other Decompositions}
\subsection{Polar Decomposition} 