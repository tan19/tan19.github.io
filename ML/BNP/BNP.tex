\documentclass{article}
\input{/Users/tan19/Dropbox/LaTeXMacros.tex}

\title{Bayesian Nonparametrics Notes}
\author{Xi Tan (xtan3.1415926@gmail.com)}
\date{\today}

\begin{document}
\maketitle
\tableofcontents


\section{Introduction}
This note is based on Peter Orbanz's BNP notes:
\vspace*{5mm}
\\
\url{http://stat.columbia.edu/~porbanz/npb-tutorial.html}

\section{Notation}
Bold upper case letters represent matrices, e.g., $\X, \Y, \Z, \bTheta$. Bold lower case letters represent vector-valued random variables and their realizations (we do not distinguish between the two), e.g., $\x, \y, \z, \btheta$. Curly upper case letters represent spaces (i.e., possible values) of random variables, e.g., $\XX, \YY, \ZZ, \Theta$.

\section{Terminology}
\subsection{Parametric and nonparametric models}
In a set of probability spaces $\{(\YY, \FF, \PP_\Theta)\}$, a {\em{statistical model}} $\MM$ on a sample space $\YY$ is {\underline{a set of probability measures}} $\PP_\Theta$ on $\YY$. If we write $PM(\YY)$ for the space of all probability measure on $\YY$, a model is a subset $\MM \subset PM(\YY)$. Every element of $\MM$ has a one-to-one mapping (hence the model is {\em{identifiable}}) with its parameter $\btheta$ with values in a parameter space $\Theta$, that is,
\begin{align}
\MM(\y) = \{P_\btheta(\y) | \btheta \in \Theta\},\quad \y \in \YY.
\end{align}
For example, a first order polynomial is a model, and a second order polynomial is another model. We can of course fit a model to the observed data, but {\em{model}} itself is an abstract concept, where the parameter values of a model need not be specified. We call a model {\em{parametric}} if $\Theta$ has finite dimension, and {\em{nonparametric}} if $\Theta$ has infinite dimension.

To formulate statistical problems, we assume that $n$ observations $\y_1,\dots,\y_n$ with values in $\YY$ are observed, which are drawn i.i.d. from a measure $P_\btheta$ in the model, i.e.,
\begin{align}
\y_1, \dots, \y_n \sim_{iid} P_\btheta \qquad \text{for some}~~ \btheta \in \Theta
\end{align}
The objective of statistical {\em{inference}} is then to draw conclusions about the value of $\btheta$ (and hence about the distribution $P_\btheta$ of the data) from the observations.


\subsection{Bayesian and Bayesian nonparametric models}
In Bayesian statistics, all parameters are considered as random variables. Hence under a Bayesian model, data are generated in two stages, i.e.,
\begin{align}
\btheta &\sim P(\btheta)\\
\y_1, \dots, \y_n ~|~ \btheta &\sim_{iid} P_\btheta(\y)
\end{align}
The objective is then to determine the {\em{posterior distribution}} -- the conditional distribution of $\btheta$ given the observed data,
\begin{align}
\pi(\btheta | \y_1, \dots, \y_n)
\end{align}
A {\em{Bayesian nonparametric}} model is a Bayesian model whose parameter space $\Theta$ has infinite dimension. To define a Bayesian nonparametric model, we have to define a prior $\pi$ on an infinite-dimensional space, which is a stochastic process with paths (i.e. realizations) in $\Theta$.

\section{Clustering and the Dirichlet process}
\subsection{Finite mixture models}
The basic assumption of a clustering problem is that each observation $\y_i$ belongs to a single cluster $k \in \{1,\cdots, K\}$, which has a cluster distribution
\begin{align}
P_k(\y_i | z_i = k)
\end{align}
where we have defined a latent variable $z_i$, indicating the cluster assignment of observation $\y_i$. Note that under the Bayesian framework, the latent variable $z_i$ itself has a distribution
\begin{align}
p_k^i \equiv P(z_i = k)
\end{align}

The marginal distribution of the observation $\y_i$ is then
\begin{align}
P(\y_i) = \sum_{k=1}^K P(z_i = k) P_k(\y_i | z_i = k)
\end{align}
A model of this form is called a {\em{finite mixture model}}.

\subsection{Bayesian mixture models}
Suppose we know there are $K$ clusters, we first sample the cluster parameters from some base measure:
\begin{align}
\btheta_1, \dots, \btheta_K \sim_{iid} G(\bbeta)
\end{align}
We then independently sample the latent cluster assignment vectors and the actual observations:
\begin{align}
(p_1^i, \dots, p_K^i) &\sim \text{Dirichlet}_K(\alpha)\\
z_i &\sim \text{Categorical}(p_1^i, \dots, p_K^i)\\
\y_i &\sim P_k(\y_i | \btheta_k, z_i = k)
\end{align}

\subsection{Dirichlet Process}
\deff{
	If $\alpha > 0$ and if $G$ is a probability measure on $\Omega_\phi$, the random discrete probability measure $\Theta$ generated by
	\begin{align}
	V_1, V_2, \dots \sim_{iid} \text{Beta}(1,\alpha)\\
	C_k = V_k\prod_{j=1}^{k-1}(1-V_k)\\
	\Phi_1, \Phi_2, \dots \sim_{iid} G
	\end{align}
	is called a {\em{Dirichlet process (DP)}} with base measure $G$ and concentration $\alpha$, and denote its law by $\text{DP}(\alpha, G)$.
}




\section{Latent features and the Indian buffet process}


\end{document}
